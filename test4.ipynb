{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":324331,"status":"ok","timestamp":1680538351096,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":-210},"id":"JhltIp--N23J","outputId":"a2fe77ca-1ffd-4b6a-fd3e-b697899491b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1680538351098,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":-210},"id":"G2f-YUUVsA4V","outputId":"4250dba7-fddb-457b-fc96-2a4c5e24a399"},"outputs":[{"output_type":"stream","name":"stdout","text":["X\n"]}],"source":["print(\"X\")"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25294,"status":"ok","timestamp":1680549307753,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":420},"id":"u2ylSDU1ZKME","outputId":"e9eeeaa9-6f2a-4345-aa66-18c853da6062"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentence_transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting transformers<5.0.0,>=4.6.0\n","  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (4.65.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (2.0.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.15.1+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.22.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.10.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (3.8.1)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub>=0.4.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.10.7)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (3.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (1.1.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (8.1.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n","Building wheels for collected packages: sentence_transformers\n","  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=2c7851cde2634ebcf96e2a988738aba7195a56d40d85bf2331483fe51e80f873\n","  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n","Successfully built sentence_transformers\n","Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence_transformers\n","Successfully installed huggingface-hub-0.13.3 sentence_transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.27.4\n"]}],"source":["!pip install sentence_transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6883,"status":"ok","timestamp":1680549592145,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":420},"id":"GPah1Z9cY_cS","outputId":"c98ca8a6-8aa2-4a0a-c6d7-f9291f330ee8"},"outputs":[{"output_type":"stream","name":"stdout","text":["embeddings1\n","[[ 2.88247615e-02 -6.02380000e-03 -5.94700649e-02 -5.17469868e-02\n","   2.10384782e-02  2.61498112e-02 -1.85921472e-02 -4.84943576e-02\n","  -6.62563294e-02 -1.73992589e-02 -1.97353140e-02  5.96294133e-03\n","  -4.01099361e-02 -2.48470530e-03  2.69772783e-02 -1.29419984e-02\n","   1.14891138e-02 -1.23000471e-03  3.06768101e-02 -5.87955117e-02\n","  -4.33826186e-02 -2.26885099e-02  1.49796642e-02  4.47340980e-02\n","  -3.10832988e-02 -5.16841561e-03 -5.26350066e-02 -4.70695794e-02\n","  -2.69006602e-02 -2.04097219e-02 -3.36187184e-02 -7.67815858e-03\n","  -3.76767777e-02  5.11414036e-02 -1.73907727e-02 -5.26173376e-02\n","   6.12982884e-02 -4.62199524e-02  5.29343002e-02  3.41578713e-03\n","   1.01563130e-02 -1.33060720e-02  1.42490435e-02  4.86509539e-02\n","  -3.87502909e-02 -3.60781476e-02 -5.88451959e-02  5.76087162e-02\n","  -8.70619994e-03 -2.85297465e-02 -5.38518988e-02 -5.06362878e-02\n","  -5.66152260e-02 -5.23519441e-02 -5.51528148e-02 -5.93431480e-02\n","   1.70422848e-02 -2.84925140e-02 -3.21431011e-02 -4.30418439e-02\n","   1.95508096e-02 -2.15407331e-02 -3.91839258e-02 -6.20332323e-02\n","  -4.30319495e-02 -6.20465204e-02  3.77914570e-02  9.05106775e-03\n","  -4.88603674e-02 -3.46687324e-02 -1.99719947e-02  2.95460504e-02\n","  -2.39549624e-03  2.09401380e-02 -3.58091742e-02 -2.34029610e-02\n","  -3.80880875e-03  1.03340335e-02 -3.47284973e-02  8.46471824e-03\n","   1.54660149e-02  2.96391989e-03  4.88317274e-02  3.57534029e-02\n","  -4.55225483e-02 -1.34987747e-02 -2.67433971e-02 -3.70878689e-02\n","   1.35265850e-02 -2.43106857e-02 -2.88488530e-03 -2.93228608e-02\n","  -7.29557220e-03  9.18805785e-03 -2.01636162e-02 -4.49694693e-02\n","  -6.23867214e-02 -6.08898103e-02  7.75439292e-03 -5.03804423e-02\n","  -3.29925702e-03  8.18348117e-03  3.41267958e-02 -3.53734456e-02\n","   5.16564678e-03 -6.49882406e-02 -2.66351068e-04 -3.48331407e-02\n","  -4.32851054e-02 -2.28978992e-02 -6.90954328e-02 -1.31462868e-02\n","  -3.04786433e-02 -3.16719264e-02  7.58232269e-03 -4.02082503e-02\n","  -3.10464520e-02 -3.89732569e-02  2.23260932e-03 -9.73848812e-03\n","   3.16375494e-02 -3.04396618e-02 -2.89071891e-02  3.72123532e-02\n","   4.14969958e-02 -4.61897887e-02 -2.50755493e-02  3.05693261e-02\n","  -5.30358963e-02 -6.68826178e-02 -6.16482981e-02  1.68833043e-02\n","  -5.22676744e-02 -1.10760592e-02 -1.41237285e-02 -1.63569371e-03\n","  -3.08979489e-03 -3.78345326e-02 -1.98838674e-02  1.20192561e-02\n","   1.38286566e-02 -6.26147389e-02 -1.88191142e-02 -1.62903368e-02\n","  -3.50699909e-02 -7.33695852e-05 -6.01761565e-02  3.62949520e-02\n","  -2.34729629e-02 -6.41112030e-02 -6.08773492e-02 -1.15935784e-03\n","   2.48932466e-02 -5.24315760e-02  2.83661373e-02 -5.06549627e-02\n","  -1.13367029e-02 -5.96601181e-02 -2.79914541e-03 -8.62470549e-03\n","   3.72227542e-02  3.14817950e-02 -6.17926791e-02  1.95854413e-03\n","   4.55179214e-02  3.82708013e-02 -4.20402968e-03 -8.93289130e-03\n","  -6.11456186e-02  5.33335358e-02  3.37866507e-02 -2.69183423e-02\n","  -2.14896910e-02 -5.16624674e-02  3.45843076e-03 -3.33708748e-02\n","  -2.95250900e-02  6.07657246e-03 -5.37878014e-02 -9.99658741e-03\n","   3.18192248e-03 -1.11188767e-02 -1.82320494e-02 -4.45552394e-02\n","  -5.15931984e-03  6.89782808e-03 -1.53336246e-02 -3.63991000e-02\n","  -4.80286814e-02 -1.31378779e-02 -3.09237670e-02  5.17635196e-02\n","  -5.58981113e-02  4.45452109e-02 -4.13625240e-02  7.98765849e-03\n","   3.13749276e-02  5.08464985e-02 -5.18147983e-02  2.72644125e-02\n","  -2.21739672e-02  2.30282582e-02 -5.98272644e-02 -4.79650721e-02\n","  -5.74542768e-02 -1.81901990e-03 -1.94741525e-02  1.19789047e-02\n","  -4.30949107e-02 -6.36261180e-02 -4.31678854e-02 -1.01506980e-02\n","  -3.06980591e-02 -1.10906037e-02 -3.95290777e-02 -4.81206588e-02\n","  -6.94997460e-02 -1.77399814e-02  9.77704581e-03 -4.65088412e-02\n","   5.60861127e-03  6.44145012e-02  2.93046273e-02  2.13886611e-02\n","  -2.50809062e-02 -1.41142150e-02 -4.01468240e-02  3.66947032e-03\n","  -8.58880952e-03 -4.12877090e-02 -5.36339842e-02  1.33703528e-02\n","  -5.58610111e-02 -2.47946084e-02 -2.59810910e-02 -5.83006404e-02\n","   2.82324869e-02 -6.37476668e-02  5.62352650e-02 -6.15695119e-02\n","   3.21000703e-02 -3.20052430e-02  1.36179440e-02 -1.86227541e-02\n","   2.42476561e-03 -6.06243685e-02 -3.75421122e-02  6.11945279e-02\n","   4.78550829e-02 -6.30280748e-02  4.03410234e-02 -6.33457229e-02\n","  -6.70027584e-02  7.45475572e-03 -5.71369715e-02  2.79410295e-02\n","  -6.44824952e-02  4.22500223e-02 -4.49701361e-02 -5.21670915e-02\n","   2.35881768e-02 -1.49932560e-02  3.50820236e-02 -5.37457392e-02\n","  -1.79608390e-02 -5.07363789e-02 -3.35810967e-02 -2.78317668e-02\n","  -1.56532861e-02  2.62663350e-03 -4.22525629e-02  3.46278325e-02\n","  -9.10688937e-03 -3.77389416e-02 -4.74276133e-02 -5.22579961e-02\n","  -5.04327081e-02 -4.10727784e-02 -2.38906522e-03  1.09453378e-02\n","  -9.90917720e-03 -3.48338708e-02 -6.50514886e-02 -2.32396293e-02\n","  -3.53580527e-02 -5.65564074e-02  4.24334593e-02 -4.06940319e-02\n","  -1.21616535e-02  2.54570656e-02 -6.38519898e-02 -7.00477697e-03\n","   5.34320949e-04  2.57173972e-03 -2.01279018e-02 -6.01199344e-02\n","  -8.78899917e-03  2.81375721e-02 -3.26352678e-02 -9.89701599e-03\n","  -4.55769971e-02  4.33933847e-02 -3.97695601e-02 -2.65960470e-02\n","   5.01693077e-02  2.87556555e-02 -2.08015740e-02  4.42582145e-02\n","   1.37913842e-02 -2.81131417e-02 -2.27111671e-02 -7.94412382e-03\n","  -4.18045223e-02  7.61975115e-03 -1.63606890e-02 -3.34824137e-02\n","  -5.56864925e-02  1.88100599e-02  5.75350933e-02 -4.47768606e-02\n","  -3.15775760e-02 -2.29821866e-03  3.35797146e-02  1.14902444e-02\n","  -3.62124220e-02  3.06749009e-02 -6.45889714e-02  4.74625221e-03\n","  -5.23945205e-02  1.44097861e-02 -3.29621439e-03 -1.33495107e-02\n","   3.73822786e-02 -8.16507731e-03  1.89212635e-02 -2.23452952e-02\n","   4.10222411e-02  4.24485505e-02 -4.88692857e-02 -4.72017974e-02\n","  -4.47561666e-02  3.89430411e-02  1.02501893e-02 -2.29442567e-02\n","   1.89856291e-02 -4.06269953e-02 -4.70609665e-02 -1.72494147e-02\n","  -8.94225109e-03  1.03158494e-02 -6.19794354e-02  1.78492386e-02\n","   1.75861847e-02 -1.93631165e-02  2.46312078e-02 -3.11326352e-03\n","   5.47488257e-02 -5.94109558e-02  1.38999037e-02 -4.23499234e-02\n","  -5.40985949e-02 -6.43414780e-02 -5.59754632e-02 -5.75860441e-02\n","  -6.39189556e-02 -2.64881104e-02 -1.04666920e-02  1.63386818e-02\n","   3.98190841e-02  2.77355760e-02  2.63460688e-02 -2.12516133e-02\n","  -2.53099296e-02 -2.32132599e-02 -2.44508591e-03 -5.68822362e-02\n","   1.39427083e-02 -6.37865514e-02 -6.75248280e-02 -1.46060465e-02\n","   6.35368330e-03 -2.06025150e-02  6.38724910e-03  2.79664751e-02\n","  -3.74202169e-02 -1.94878504e-02 -5.46371974e-02 -1.88382324e-02\n","  -5.17937206e-02 -4.51066829e-02  1.34901155e-03  1.15912305e-02\n","  -4.89017442e-02  1.77479461e-02  8.72040540e-03 -2.57275626e-02\n","   2.01339219e-02 -1.69508904e-02 -1.16100884e-03 -3.55251543e-02\n","   5.15420102e-02 -1.03644626e-02 -4.71463464e-02 -4.15852442e-02\n","   3.84895771e-04 -5.73048331e-02 -1.24670705e-02 -1.60675757e-02\n","  -6.88861012e-02 -2.04198458e-03  2.45294888e-02 -2.62926910e-02\n","  -5.65543957e-02 -5.44632040e-02 -6.14586323e-02 -5.05635366e-02\n","  -6.92791343e-02 -2.96370666e-02 -2.29854155e-02 -4.02796045e-02\n","   1.45851681e-02 -7.51706306e-03 -6.27588108e-02 -6.89986488e-03\n","   2.54288572e-03 -5.06707318e-02 -3.92588489e-02 -4.27210191e-03\n","   4.41401079e-02 -5.22606745e-02  5.80937192e-02 -5.62397018e-02\n","  -6.43873364e-02 -4.61812951e-02 -6.21675849e-02  4.62303869e-02\n","  -1.44763160e-02  1.40188951e-02 -1.64143089e-02  2.91948039e-02\n","   1.50602860e-02 -2.21326528e-03  2.44559497e-02  1.25198979e-02\n","  -3.74395475e-02  7.32105505e-03 -6.49807900e-02 -6.82402328e-02\n","  -1.01269018e-02 -4.04962897e-02 -5.37482183e-03  1.26281139e-02\n","   5.36094494e-02 -5.28344959e-02 -5.84868938e-02 -9.12231393e-03\n","   1.51850758e-02  1.07937269e-02  1.60402190e-02 -5.07205762e-02\n","   4.57695387e-02 -1.40128424e-02  2.82719303e-02  3.35106477e-02\n","  -3.49350017e-03 -5.77242449e-02  4.94684950e-02  4.08659136e-04\n","  -5.71468249e-02  3.65157910e-02 -3.85552198e-02 -1.54661387e-02\n","  -2.97778305e-02 -5.65411896e-02 -4.54071574e-02  5.66024594e-02\n","  -2.83147730e-02 -5.38148955e-02 -5.73222041e-02  5.59311882e-02\n","  -3.95889245e-02  2.21080743e-02 -6.51831850e-02 -4.59038690e-02\n","   1.97471846e-02 -3.46787721e-02 -5.09557128e-02  1.25375697e-02\n","   5.14526926e-02 -6.65719807e-02 -6.97930297e-03 -3.91318388e-02\n","  -5.54135479e-02 -2.02546492e-02 -2.31055189e-02 -5.43999448e-02\n","  -4.59519885e-02 -2.75442209e-02  1.89669442e-03 -2.78644785e-02\n","  -1.36639951e-02 -1.77377090e-02 -6.35466054e-02  4.31045853e-02\n","  -1.06591154e-02 -5.34900390e-02  9.45197791e-03 -1.59898009e-02\n","  -5.05440263e-03 -5.80852479e-02 -2.90006325e-02 -5.44504263e-02\n","   6.49545109e-03 -1.12385461e-02 -2.01125015e-02  1.49725918e-02\n","   4.32839096e-02 -5.11247814e-02  5.50761446e-03  5.63070402e-02\n","  -1.92828663e-02 -2.60010231e-02  3.59329954e-02 -3.91544215e-02\n","  -1.78889837e-02 -2.41307393e-02  1.89416315e-02 -4.23858352e-02\n","   2.38775387e-02 -6.31673634e-02  5.58254309e-04  2.51872819e-02\n","  -1.32580614e-02 -7.84787908e-03  3.54420729e-02 -4.91291173e-02\n","   2.47864313e-02 -1.49373617e-02 -6.08817711e-02 -3.64215374e-02\n","  -5.00710532e-02 -1.83397215e-02  9.28421505e-03 -2.14534768e-04\n","   1.32443327e-02 -3.64747196e-02 -3.91296595e-02 -6.45212084e-02\n","   9.48439259e-03 -6.83205724e-02 -2.49880161e-02  1.14904912e-02\n","  -2.21833400e-03 -2.34719049e-02 -2.84797121e-02 -3.52672231e-03\n","  -4.42356579e-02 -2.62395199e-02 -4.81571481e-02 -1.54173477e-02\n","   1.55678494e-02  1.87349990e-02  3.96036990e-02 -5.81524745e-02\n","  -2.93157771e-02  2.69755181e-02  4.36863415e-02  4.14391756e-02\n","   6.14666715e-02  3.39967012e-02  8.20977055e-03 -1.27511146e-02\n","   1.02568539e-02  4.22609635e-02 -7.11092632e-03  3.02006695e-02\n","  -5.79457916e-02 -1.15043335e-02  3.34275933e-03 -3.36461999e-02\n","  -4.73092198e-02 -3.62695716e-02 -4.34916951e-02 -2.45492235e-02\n","  -3.63183506e-02 -1.78291630e-02 -2.12953072e-02 -4.56005260e-02\n","   4.28570621e-02 -3.93728651e-02  2.01759692e-02  2.62516197e-02\n","  -6.36575967e-02  2.40485463e-03 -2.14615017e-02 -3.84575762e-02\n","   5.58038168e-02 -3.71750370e-02  1.49209341e-02 -4.12983727e-03\n","   1.86757895e-03 -2.04829685e-02 -1.04089966e-02 -3.29537615e-02\n","  -1.66058652e-02 -3.98148708e-02 -1.44136027e-02  1.73095576e-02\n","  -1.25491768e-02 -2.31534988e-02  2.51558051e-03 -5.44090420e-02\n","   3.85257043e-02  9.91111714e-03 -1.76963415e-02  4.13155816e-02\n","   3.43245678e-02 -5.44645935e-02 -2.50790324e-02  9.89234168e-03\n","  -4.91984487e-02 -2.33771726e-02  4.84473864e-03 -4.29995246e-02\n","   6.24709167e-02 -4.83986624e-02  1.50447749e-02 -9.95594263e-03\n","   4.04974930e-02 -4.35253493e-02  4.93707582e-02 -8.19090661e-03\n","   1.03982817e-02 -2.92950980e-02 -4.80249897e-02 -4.04862203e-02\n","  -3.99036668e-02 -5.85782379e-02  1.90752149e-02 -1.16002141e-02\n","  -4.24260348e-02 -4.78405394e-02  1.78162754e-02 -2.55294014e-02\n","  -2.47923974e-02  5.48832258e-03  3.40621336e-03  4.35627019e-03\n","  -4.45669182e-02  4.04864177e-03 -5.69999442e-02 -6.74914755e-03\n","   2.73631811e-02 -8.40749405e-03 -2.41456255e-02 -4.96040583e-02\n","  -1.87330730e-02  3.88434231e-02 -4.61333282e-02  1.63569693e-02\n","   3.33762318e-02  2.98068114e-02 -1.58269964e-02  5.63242547e-02\n","  -2.97456682e-02 -5.40858768e-02 -8.49057268e-03 -6.70826361e-02\n","   4.07349458e-03  2.03122422e-02  3.01039163e-02 -2.28476860e-02\n","  -2.95510050e-02 -5.21667860e-02 -6.86138123e-02 -4.05925401e-02\n","  -2.15916356e-04 -3.57156806e-02 -7.02572428e-03  3.17010060e-02\n","  -1.50282625e-02 -4.44737598e-02  2.60928366e-02 -4.59461994e-02\n","  -4.79453988e-03  4.86959256e-02  9.05089825e-03 -3.21240798e-02\n","  -6.56551041e-04 -2.12887097e-02 -1.99807640e-02  1.22920685e-02\n","  -6.60480857e-02  1.74112862e-03 -5.42488880e-02  4.88857320e-03\n","  -4.13853452e-02 -4.29894924e-02 -7.14762660e-04 -1.17400400e-02\n","   1.17806392e-02 -5.30080050e-02  5.83433034e-03 -3.31776775e-02\n","   1.25234644e-03 -1.68362074e-02  7.85127468e-03 -5.75666092e-02\n","   4.75413948e-02 -4.88852784e-02 -2.48548668e-02 -2.64456589e-02\n","   1.94522273e-02 -1.55941918e-02 -4.10943143e-02 -5.01935184e-02\n","   1.31321093e-02 -3.77094001e-02 -4.83529596e-03 -1.67820267e-02\n","  -1.03456283e-03  4.36241738e-02  1.22175915e-02  1.41191566e-02\n","  -4.64913808e-03  1.51507305e-02  7.27053639e-03 -2.64825262e-02\n","  -4.01400588e-02  4.87504192e-02 -5.08385636e-02  3.84581499e-02\n","   1.49936136e-02 -2.22761370e-02 -2.51135193e-02 -5.59066050e-02\n","  -1.21982833e-02 -1.62783228e-02 -3.20170261e-02 -3.28726284e-02\n","   1.10821584e-02  3.40707190e-02 -4.90031689e-02  1.84660070e-02\n","  -3.98736447e-03 -4.45254780e-02  1.93904135e-02 -2.96415016e-02\n","  -4.00838815e-02 -1.30894631e-02  2.36908067e-02  3.50188576e-02\n","   4.16627005e-02  1.94516790e-03 -3.81276011e-02 -3.38427685e-02\n","  -4.40779850e-02 -3.13737728e-02  5.56564145e-02 -5.60914837e-02\n","  -3.60132568e-02  4.59217541e-02  1.84080340e-02 -2.36960612e-02\n","   7.01344479e-03 -3.00224945e-02 -2.96070166e-02  6.74804673e-04]]\n","--------------\n","embeddings2\n","[[ 4.82903123e-02 -2.77337674e-02 -5.98193705e-02 -5.65644354e-02\n","   1.02113634e-02  3.08120567e-02 -2.49280334e-02 -5.18251918e-02\n","  -6.30539507e-02 -2.34919805e-02 -2.23107301e-02  7.00214552e-03\n","  -2.36975979e-02 -1.02376323e-02  2.91547123e-02 -2.25248672e-02\n","   1.43828103e-03 -3.83340660e-03  2.10842434e-02 -5.85178807e-02\n","  -3.25632803e-02 -1.01931272e-02 -1.86110195e-02  3.77945080e-02\n","  -3.33677940e-02 -1.12271458e-02 -4.14455272e-02 -2.84359735e-02\n","  -3.70374396e-02 -2.54109055e-02 -3.37858014e-02 -1.37263015e-02\n","  -4.92449552e-02  5.23888581e-02 -4.38048877e-02 -5.27704619e-02\n","   5.87747507e-02 -3.69757265e-02  3.62192690e-02 -2.83183833e-03\n","  -9.05531086e-03 -2.12336611e-02 -1.48310501e-03  4.64591831e-02\n","  -4.17727195e-02 -3.30026150e-02 -6.09106049e-02  4.69100960e-02\n","   1.58672333e-02 -2.48021800e-02 -5.13694175e-02 -5.86561188e-02\n","  -5.24315313e-02 -3.35882120e-02 -4.33190055e-02 -5.39812855e-02\n","   3.01640593e-02 -2.55274381e-02 -2.35791411e-02 -4.89599295e-02\n","   3.14057898e-03 -4.07087468e-02 -3.73945609e-02 -6.22126497e-02\n","  -5.17891608e-02 -6.08972497e-02  4.00807373e-02 -1.96862221e-03\n","  -5.25894761e-02 -3.46023701e-02 -3.11610270e-02  1.19896010e-02\n","   1.09717464e-02  2.57122125e-02 -5.18186837e-02 -7.08532659e-03\n","  -1.80033259e-02  2.64619850e-02 -4.74190637e-02  4.39481949e-03\n","  -3.00460472e-03  1.76456093e-03  3.45262252e-02  3.58404852e-02\n","  -4.36501503e-02 -2.71329749e-02 -1.56286638e-02 -3.62902060e-02\n","   1.90634094e-02 -2.19669491e-02 -1.93472169e-02 -3.71944122e-02\n","  -2.27879211e-02 -3.18697132e-02 -1.15910368e-02 -5.37592396e-02\n","  -6.22987710e-02 -6.26252145e-02  7.12449709e-03 -5.01743928e-02\n","  -1.79654593e-03 -1.43352505e-02  2.84513254e-02 -4.50953841e-02\n","  -2.15936694e-02 -6.37315810e-02  2.31895577e-02 -3.78158912e-02\n","  -5.06771430e-02 -2.63372976e-02 -6.81749508e-02 -7.95187429e-03\n","  -2.68694554e-02 -3.79936136e-02  8.46367236e-03 -4.71113622e-02\n","  -4.29871194e-02 -4.92195636e-02 -1.54084642e-03 -1.35034639e-02\n","   3.90284397e-02 -2.47448664e-02 -1.90519728e-02  6.02351676e-04\n","   4.37516458e-02 -4.91161272e-02 -3.16692367e-02  8.89349636e-03\n","  -4.57492061e-02 -6.55247793e-02 -6.11956082e-02  1.15681486e-02\n","  -5.44383042e-02 -1.27019482e-02 -1.91848259e-02 -2.39801183e-02\n","  -1.02858478e-02 -4.03553583e-02 -1.43470373e-02  2.34684572e-02\n","  -1.63851604e-02 -6.17501624e-02 -6.68099802e-03 -1.99119095e-02\n","  -3.85054424e-02 -1.70629174e-02 -5.71227744e-02  1.89183671e-02\n","  -2.63844188e-02 -6.11164123e-02 -6.29648417e-02 -6.17660070e-03\n","   1.29457898e-02 -4.55267727e-02  1.41995298e-02 -4.72532995e-02\n","  -1.72674973e-02 -6.22579306e-02 -2.38795467e-02 -2.71196570e-02\n","   2.97907777e-02  2.18953509e-02 -6.02907874e-02  1.16763609e-02\n","   4.89605404e-02  3.13902125e-02 -2.25610454e-02 -9.17146076e-03\n","  -5.83596751e-02  4.88356873e-02  3.03326938e-02 -3.03806532e-02\n","  -2.89278049e-02 -4.29488942e-02 -1.79317016e-02 -2.07674578e-02\n","  -2.48838328e-02 -1.75108400e-03 -5.07740267e-02 -3.92604098e-02\n","  -2.05416493e-02  2.02180585e-03 -5.19875903e-03 -4.97589037e-02\n","  -5.25584025e-03  3.74384574e-03 -7.10413046e-03 -3.55240740e-02\n","  -4.85055707e-02 -2.79446994e-03 -1.97678488e-02  3.81527729e-02\n","  -5.72424829e-02  4.03197631e-02 -3.44533920e-02 -5.46079827e-04\n","   3.85475904e-02  4.36939783e-02 -4.56617214e-02  1.84743013e-02\n","  -3.34793553e-02  1.78992432e-02 -6.11671619e-02 -5.05016856e-02\n","  -5.86104915e-02 -9.20523191e-04 -2.89288256e-02  3.62872891e-03\n","  -5.50038032e-02 -6.31985217e-02 -2.61533950e-02 -2.23915800e-02\n","  -3.32229175e-02 -1.21226674e-02 -3.75961624e-02 -5.57146370e-02\n","  -6.84903115e-02 -2.88624447e-02  1.56537397e-03 -5.09149693e-02\n","  -2.18388438e-02  6.18896447e-02  1.06637822e-02 -1.38463397e-02\n","  -2.28455309e-02 -1.33980615e-02 -3.96680757e-02  5.77988941e-03\n","  -8.98170471e-03 -3.42754684e-02 -4.62873764e-02  2.10439451e-02\n","  -5.94795756e-02 -2.19632834e-02 -3.34758945e-02 -5.58665246e-02\n","   3.98179749e-03 -6.41875491e-02  5.44456244e-02 -6.24976195e-02\n","   2.85442080e-02 -3.07678021e-02  9.98098869e-03 -1.91553831e-02\n","  -1.63986906e-02 -5.26492856e-02 -2.50148382e-02  5.80370799e-02\n","   2.44049970e-02 -6.40591532e-02  3.73062193e-02 -6.15979172e-02\n","  -6.58516437e-02 -1.01528130e-02 -5.92981204e-02  3.93009633e-02\n","  -6.40844479e-02  3.18427347e-02 -3.97991240e-02 -5.08190878e-02\n","  -3.84119595e-03 -1.50271496e-02  3.96785550e-02 -5.07907011e-02\n","  -1.08073105e-03 -5.45104556e-02 -3.70787904e-02 -2.79515237e-02\n","  -1.74216069e-02 -2.98900786e-03 -3.46513763e-02  2.09053233e-02\n","  -1.09938246e-05 -4.24702615e-02 -4.01960276e-02 -5.30007593e-02\n","  -4.66881022e-02 -5.09424359e-02 -4.70789848e-03  4.79511888e-04\n","  -1.92577690e-02 -4.91818115e-02 -6.06351197e-02 -4.16085608e-02\n","  -2.54131854e-02 -4.65227291e-02  1.64109971e-02 -5.16735576e-02\n","  -3.07207014e-02  2.07147617e-02 -6.15055189e-02  7.51337130e-03\n","  -1.02816233e-02  2.11121943e-02 -1.67552941e-02 -5.56973331e-02\n","  -1.80667285e-02  1.63881555e-02 -2.62553822e-02 -1.87215786e-02\n","  -4.35091294e-02  4.34251651e-02 -4.83063050e-02 -3.54561135e-02\n","   3.04502454e-02  4.62029921e-03 -2.27933228e-02  3.58008333e-02\n","   4.95574391e-03 -2.97218971e-02 -2.58790534e-02 -1.61753241e-02\n","  -5.44590876e-02  8.56934674e-03 -1.83553975e-02 -3.68114486e-02\n","  -5.69187887e-02  3.06740194e-03  5.34740873e-02 -4.62802313e-02\n","  -2.34192386e-02 -1.84462182e-02  1.60358064e-02 -5.49075892e-03\n","  -4.37200032e-02  2.82804109e-02 -6.55559972e-02 -6.18039118e-03\n","  -5.64045012e-02  2.37551387e-02 -8.61109514e-03 -3.11113857e-02\n","   2.16636304e-02 -2.36915704e-02 -6.51246076e-03 -4.58612405e-02\n","   3.26540433e-02  1.92386601e-02 -4.59985845e-02 -5.00270464e-02\n","  -4.27757613e-02  2.38440130e-02  7.12807581e-04 -3.66080627e-02\n","   1.96923800e-02 -5.13499975e-02 -3.93173546e-02 -2.71546375e-03\n","  -2.56533269e-02  1.76943429e-02 -6.41787499e-02  1.53666679e-02\n","   1.84386242e-02 -2.79593617e-02  1.68401897e-02 -2.30595246e-02\n","   4.96789441e-02 -5.90909198e-02  7.31257955e-03 -2.67537441e-02\n","  -5.02089337e-02 -6.38413206e-02 -5.56195714e-02 -5.92331402e-02\n","  -6.12214506e-02 -3.14592868e-02 -1.22686010e-02  1.38580045e-02\n","   3.96538302e-02  1.51109137e-03  1.46390488e-02 -1.02759385e-02\n","  -2.45716553e-02 -1.45771094e-02 -8.69700126e-03 -5.20778075e-02\n","  -9.15977079e-03 -5.32866307e-02 -6.61440492e-02 -1.81435253e-02\n","  -5.78493229e-04 -4.05406170e-02 -7.14121526e-03  1.99683309e-02\n","  -4.79659513e-02 -1.33408830e-02 -5.47643304e-02 -2.44666226e-02\n","  -6.00667857e-02 -5.58786578e-02 -1.98033229e-02  1.66627057e-02\n","  -4.40032147e-02  7.25193461e-03  4.44333535e-03 -3.36690322e-02\n","   1.06907645e-02 -1.59565452e-02 -1.49234815e-03 -3.12786028e-02\n","   4.07464989e-02 -2.59410646e-02 -5.37327975e-02 -3.65257896e-02\n","   4.38823877e-03 -5.97070344e-02 -1.18240034e-02 -3.12864780e-02\n","  -6.76054433e-02 -1.11412741e-02  1.19722681e-02 -2.73650773e-02\n","  -5.32878265e-02 -5.36909215e-02 -6.20955341e-02 -5.87777048e-02\n","  -6.82230815e-02 -3.76367904e-02 -4.34357189e-02 -3.64974104e-02\n","  -1.37306461e-02 -2.04516482e-02 -6.18921109e-02 -1.23328585e-02\n","  -3.62485950e-03 -5.74962273e-02 -4.10005860e-02 -1.29752215e-02\n","   2.90254373e-02 -5.03570214e-02  4.97987233e-02 -5.86283281e-02\n","  -6.53949752e-02 -4.87426855e-02 -6.16068952e-02  4.40299921e-02\n","  -1.80241205e-02 -1.50876422e-03 -2.35794540e-02  9.36074555e-03\n","  -9.45969485e-03 -7.53576914e-03 -9.79265291e-03  8.96052178e-03\n","  -4.16702479e-02  1.06916055e-02 -6.33038208e-02 -6.75180852e-02\n","  -4.54071816e-03 -3.79772410e-02 -1.18828956e-02 -2.22653523e-03\n","   5.87557182e-02 -4.75959256e-02 -5.30534610e-02 -1.51380906e-02\n","  -7.70070450e-03 -1.40143437e-02  3.54583040e-02 -5.27743697e-02\n","   4.49479520e-02 -2.25015003e-02  2.89879963e-02  3.15787829e-02\n","  -5.94582316e-03 -5.77859916e-02  3.87026407e-02 -4.23284527e-03\n","  -5.71358316e-02  3.16162333e-02 -4.60343771e-02 -2.40006801e-02\n","  -3.60481218e-02 -4.90760244e-02 -5.34284711e-02  4.59530503e-02\n","  -3.51203457e-02 -4.86366227e-02 -5.09026125e-02  4.52617519e-02\n","  -2.39236504e-02  4.18202765e-03 -6.49606064e-02 -5.03972359e-02\n","   3.45802424e-03 -4.54865322e-02 -5.23466170e-02  1.94315277e-02\n","   5.38959727e-02 -6.70816451e-02 -1.13619156e-02 -2.98028961e-02\n","  -5.94836995e-02  5.80622209e-03 -2.57306714e-02 -5.12556806e-02\n","  -4.65131551e-02 -3.92137580e-02 -2.48411167e-02 -2.44039949e-02\n","  -1.54765211e-02 -2.29678154e-02 -5.88326007e-02  3.37620415e-02\n","  -1.69166736e-02 -5.05706035e-02 -1.36039956e-02 -3.24720219e-02\n","   1.12825288e-02 -5.95471673e-02 -2.51096282e-02 -3.75054143e-02\n","  -9.73897427e-03 -2.31907014e-02 -1.59729067e-02  2.25360561e-02\n","   2.48140898e-02 -5.04654050e-02  1.14984605e-02  4.33699116e-02\n","  -2.51949448e-02 -2.44554970e-02  3.41637060e-02 -3.54471020e-02\n","  -3.69465500e-02 -3.35511342e-02  6.14411058e-03 -3.70281115e-02\n","   1.12502975e-02 -6.54417053e-02 -8.33754148e-03  2.06457986e-03\n","  -2.32842658e-02 -2.15740968e-02  4.31903638e-02 -3.39559987e-02\n","   2.18934640e-02 -1.77881755e-02 -5.26832603e-02 -4.23380621e-02\n","  -5.53571396e-02 -2.46979389e-02 -7.73765566e-03 -1.63511594e-03\n","  -4.22910322e-03 -3.86466533e-02 -2.57734451e-02 -6.20186739e-02\n","  -4.55602072e-03 -6.69508278e-02 -3.03684901e-02  3.19795907e-02\n","  -6.38534501e-03 -3.57671827e-03 -4.32162583e-02 -1.03085246e-02\n","  -5.20183593e-02 -2.54856441e-02 -4.63558398e-02 -2.86604296e-02\n","   1.72200929e-02  1.71981845e-02  1.94206834e-02 -6.13130927e-02\n","  -4.05677073e-02  1.86337549e-02  4.16780710e-02  3.01936828e-02\n","   5.55597357e-02  1.18337646e-02 -3.65961343e-03 -2.64072996e-02\n","  -4.04113950e-03  1.76024493e-02 -2.57621910e-02  1.51665984e-02\n","  -6.18067905e-02 -1.31866150e-02  9.98361409e-03 -3.68848592e-02\n","  -5.53941242e-02 -2.60629319e-02 -5.48637919e-02 -3.41939367e-02\n","  -3.59186530e-02 -1.93308014e-02 -2.01722346e-02 -5.43623753e-02\n","   3.44225504e-02 -4.16433960e-02  2.49207951e-02  1.94237493e-02\n","  -6.20779470e-02 -1.71263225e-03 -1.84476227e-02 -4.77674380e-02\n","   5.39959520e-02 -4.34958637e-02 -8.43759160e-03  4.12664423e-03\n","  -8.46223193e-05 -2.23057158e-02 -6.11298719e-06 -2.80724950e-02\n","  -2.56095026e-02 -3.24865878e-02 -2.12791096e-02  1.64833870e-02\n","  -2.52143890e-02 -3.93823609e-02 -1.11922463e-02 -3.86641473e-02\n","   3.91902588e-02  1.17510650e-02 -6.10805396e-03  3.62521447e-02\n","   2.46790852e-02 -5.24723604e-02 -1.18484600e-02 -8.19840562e-03\n","  -4.62487601e-02 -3.16922404e-02  5.64224785e-04 -5.32029085e-02\n","   5.64947799e-02 -4.84056734e-02  9.69847385e-03  1.93694769e-03\n","   2.78808922e-02 -4.88737188e-02  3.92339230e-02 -2.55952924e-02\n","   5.10797545e-04 -3.37514393e-02 -5.39585836e-02 -3.96952555e-02\n","  -4.50422466e-02 -6.13672622e-02  1.30339444e-03 -1.75185520e-02\n","  -4.80446890e-02 -5.18635586e-02 -4.60527698e-03 -4.01407965e-02\n","   1.09957810e-03 -3.96143161e-02 -5.49913570e-03 -3.16790827e-02\n","  -4.98148948e-02 -5.60807623e-03 -5.76209240e-02 -2.11869460e-03\n","   2.05330569e-02 -1.64201781e-02 -2.29569748e-02 -4.64560576e-02\n","  -3.04182805e-02  3.93538550e-02 -5.56773096e-02 -7.96950143e-03\n","   3.73183787e-02  1.44159384e-02 -1.14091597e-02  4.82521281e-02\n","  -3.61050703e-02 -5.61515652e-02 -9.23930109e-03 -6.42844737e-02\n","  -1.06617250e-02  2.01471839e-02  8.62149987e-03 -2.73477528e-02\n","  -1.64986532e-02 -3.84157225e-02 -6.76975399e-02 -4.07050550e-02\n","  -1.71645358e-02 -4.26529385e-02 -3.16445604e-02  2.07410604e-02\n","  -5.45174396e-03 -4.50100303e-02  8.67116265e-03 -3.96936648e-02\n","  -2.65752189e-02  3.49286571e-02  4.36894363e-03 -3.38977501e-02\n","  -1.83597521e-03 -4.32720147e-02 -3.87739614e-02 -3.65139777e-03\n","  -6.56061247e-02 -2.10165028e-02 -5.48937060e-02 -1.59008652e-02\n","  -4.85912263e-02 -5.19881323e-02 -1.54770156e-02 -2.07578540e-02\n","   3.71594564e-03 -5.33723906e-02  4.38456424e-03 -3.85986045e-02\n","  -2.11116858e-02 -1.99766960e-02  7.14984140e-04 -5.86713329e-02\n","   4.07217257e-02 -5.30337058e-02 -4.41026017e-02 -4.21952158e-02\n","   1.63519196e-02 -1.33155053e-02 -4.64979596e-02 -5.58758080e-02\n","   4.05538781e-03 -3.92868854e-02 -5.39771700e-03 -3.43346447e-02\n","  -1.05512387e-03  4.12784219e-02  1.44869450e-03  2.29058918e-02\n","  -1.36254663e-02  1.16582392e-02  1.31178936e-02 -2.73631290e-02\n","  -3.93597707e-02  4.71430197e-02 -5.53393066e-02  3.41915376e-02\n","   1.01840291e-02 -4.76347208e-02 -1.80388819e-02 -5.84858730e-02\n","  -2.04186961e-02 -9.68365930e-03 -4.02262844e-02 -4.37903851e-02\n","   1.85963716e-02  1.59745049e-02 -4.34405059e-02  3.86778871e-03\n","  -9.21439752e-03 -4.24054042e-02  4.37409505e-02 -2.21645422e-02\n","  -2.72221565e-02 -7.78274192e-03  1.29527375e-02  2.71570645e-02\n","   2.24411115e-02  1.53610657e-03 -4.24765609e-02 -4.80023138e-02\n","  -4.24962565e-02 -4.05286178e-02  5.58051839e-02 -5.23247197e-02\n","  -4.76348773e-02  3.64182107e-02  1.44034224e-02 -2.65425127e-02\n","  -5.46497852e-03 -2.63813045e-02 -3.01895123e-02  4.52581933e-03]]\n","--------------\n","embeddings3\n","[[ 3.95356752e-02 -5.06109931e-02 -6.43331036e-02 -4.80635129e-02\n","   3.23022418e-02  3.29945832e-02  6.58234255e-03 -3.84778157e-02\n","  -6.81849644e-02 -2.03063302e-02 -1.77861266e-02 -1.51270945e-02\n","  -1.54627291e-02 -2.78642382e-02  4.74948846e-02 -5.95630035e-02\n","  -3.73876956e-03 -4.76210229e-02 -3.63918673e-03 -5.37701398e-02\n","   6.36402657e-03 -3.40556838e-02 -3.81683256e-03 -1.43135274e-02\n","  -1.96651146e-02  1.05870599e-02 -4.12928276e-02 -2.43671816e-02\n","  -2.46524345e-02 -4.06738520e-02 -4.07101996e-02 -2.17959996e-06\n","  -3.98964137e-02  1.93797629e-02 -4.04905789e-02 -3.84471342e-02\n","   5.62851951e-02 -5.29618897e-02  3.52967978e-02 -4.53699864e-02\n","   1.55689977e-02 -9.93163232e-03  2.18592603e-02  1.99366156e-02\n","  -3.90412286e-02 -2.08977088e-02 -5.81193753e-02  4.28439192e-02\n","  -2.54511070e-02 -1.21892709e-02 -5.79871237e-02 -4.79044691e-02\n","  -4.65916023e-02 -4.24132049e-02 -2.28395313e-02 -5.14443479e-02\n","  -3.57510559e-02 -4.31611016e-02 -4.26946357e-02 -2.54358426e-02\n","  -3.06720138e-02 -5.59493005e-02 -2.08474584e-02 -6.15655817e-02\n","  -5.03862463e-03 -5.03274053e-02  2.24803425e-02 -2.60233413e-02\n","  -5.71955629e-02 -5.43587022e-02  3.07813082e-02 -8.65681935e-03\n","   1.76331811e-02  3.03469785e-02 -5.87199479e-02 -1.54064279e-02\n","  -1.09092537e-02  1.66255552e-02 -4.16795649e-02  3.20486166e-02\n","  -1.46871759e-02  4.26576659e-03  2.63317339e-02  1.14982706e-02\n","  -4.73577082e-02 -4.84663732e-02 -7.52790924e-03 -5.50781004e-02\n","   3.29190819e-03 -2.05262024e-02 -4.97720717e-03 -3.45169939e-02\n","   1.86607596e-02  3.36755905e-03 -4.10292260e-02 -5.46876714e-02\n","  -6.44851997e-02 -4.15532999e-02  1.58766322e-02 -4.14449237e-02\n","  -4.98186098e-03  3.58956424e-03  2.29764706e-03 -3.79381888e-02\n","  -4.03407356e-03 -6.62101060e-02 -3.33671346e-02 -1.89759377e-02\n","  -3.56361605e-02 -4.88218069e-02 -6.76085502e-02 -6.96628401e-03\n","  -3.33513180e-03 -3.43769304e-02 -1.28924828e-02 -3.03905103e-02\n","  -5.18070646e-02 -6.27877116e-02  3.91544588e-03 -1.81221720e-02\n","   2.93065533e-02 -3.75711955e-02 -2.40786523e-02  8.35974887e-03\n","   4.89665531e-02 -2.73817256e-02 -3.34042087e-02  9.04205348e-03\n","  -4.53798920e-02 -5.75451702e-02 -5.39827012e-02  2.19124332e-02\n","  -5.73019646e-02  9.62908007e-03  3.98498960e-03 -1.60248298e-02\n","  -3.44073810e-02 -2.98894793e-02 -3.13385949e-02 -1.81393176e-02\n","  -3.60421184e-03 -5.52146733e-02 -3.22584286e-02  6.63033826e-03\n","  -5.50550297e-02 -9.92118567e-03 -2.80215405e-02  1.70146651e-03\n","  -5.05152494e-02 -4.97939661e-02 -6.05194718e-02  5.38192801e-02\n","   3.03801019e-02 -5.30202612e-02  4.01173159e-03 -5.23584262e-02\n","  -6.55852864e-03 -6.41949698e-02  6.56477502e-03 -3.70687321e-02\n","   1.31526822e-02  1.84152350e-02 -6.51411265e-02 -1.36529589e-02\n","   2.26479582e-02 -2.92295055e-03 -2.64573861e-02 -8.03278759e-03\n","  -4.34085950e-02  4.86192554e-02  2.36285590e-02 -3.18382159e-02\n","  -8.89257900e-03 -1.18846269e-02 -4.88561690e-02  3.12272832e-02\n","  -8.45375191e-03 -1.36818448e-02 -6.11014590e-02 -3.81485038e-02\n","   1.62738711e-02  1.00584012e-02 -3.30567025e-02 -3.23743746e-02\n","  -2.56723482e-02  1.05361678e-02 -9.83407348e-03 -3.95417176e-02\n","  -5.09486347e-02 -4.16780682e-03 -2.09944919e-02  1.96162630e-02\n","  -2.25914102e-02  6.18856121e-03 -5.31267524e-02  1.29427407e-02\n","   1.32137584e-02  3.05543076e-02 -3.17216888e-02 -2.89879162e-02\n","  -2.57572136e-03  2.44397894e-02 -6.49477169e-02 -4.78180461e-02\n","  -6.49776757e-02  1.98851302e-02 -9.98109207e-03 -5.16741537e-02\n","  -2.63123102e-02 -5.62815145e-02 -6.32614791e-02 -2.92494576e-02\n","  -4.79717702e-02 -2.39353813e-02 -3.17591652e-02 -1.28073255e-02\n","  -6.87084273e-02  4.62218840e-03  3.44866375e-03 -4.47455682e-02\n","  -1.51589029e-02  4.34789397e-02  1.81855243e-02  1.71573143e-02\n","  -7.75521994e-03  3.15460353e-03 -4.43886444e-02  2.19934769e-02\n","  -2.31446233e-02 -5.11110872e-02 -3.15020606e-02  2.10419726e-02\n","  -5.76776601e-02 -4.54903534e-03 -2.56386008e-02 -5.11766039e-02\n","   1.12748714e-02 -6.00058697e-02  5.21591604e-02 -4.70254309e-02\n","   4.25900295e-02 -2.96325181e-02  4.48875837e-02 -1.53052332e-02\n","   6.26546051e-03 -5.95483966e-02 -3.29970084e-02  5.70229217e-02\n","  -3.65729071e-02 -5.73457517e-02 -1.53041147e-02 -6.51792884e-02\n","  -6.80153221e-02 -4.32822108e-02 -5.01855761e-02  1.77228730e-02\n","  -5.51074706e-02 -2.39318386e-02 -2.24030036e-02 -4.40826453e-02\n","   7.28865108e-03  1.44336214e-02  2.27155406e-02 -4.50858995e-02\n","   1.94013945e-03 -5.66681363e-02 -4.89295796e-02 -3.97568978e-02\n","   1.72483306e-02  3.72610725e-02 -3.38172875e-02  3.07457685e-03\n","  -1.92492530e-02 -2.76052095e-02 -2.99497247e-02 -4.91800569e-02\n","  -6.38394430e-02 -4.70606498e-02  1.82983689e-02 -3.59498709e-02\n","  -5.16212508e-02 -1.25145847e-02 -6.41708151e-02 -3.28647941e-02\n","  -4.19421382e-02 -5.76383248e-02  4.80135754e-02 -3.76689918e-02\n","  -2.62211356e-02  3.53302285e-02 -5.78708313e-02  1.05503118e-02\n","  -6.26536598e-03  1.15083642e-02 -4.21916507e-02 -5.76108024e-02\n","  -2.86952872e-02  3.70795280e-02 -1.04159536e-02  5.32100722e-03\n","  -5.97776249e-02  2.12486628e-02 -5.38876988e-02  6.19841367e-03\n","   6.43742783e-03  2.65039820e-02  7.97461066e-03 -6.22506626e-03\n","  -1.34745976e-02 -2.34018024e-02  9.21894424e-03 -4.80639515e-03\n","  -5.00778593e-02 -9.56184324e-03 -9.46583677e-05 -4.36787866e-02\n","  -4.71408516e-02  6.40285620e-03  5.54967523e-02 -5.86124808e-02\n","  -7.41885509e-03 -2.50426847e-02 -7.66305812e-03 -4.74360026e-03\n","  -6.90619787e-03  1.15935924e-03 -6.40021414e-02  2.48680823e-02\n","  -5.41765876e-02 -4.03576531e-03 -2.54323911e-02 -2.73939464e-02\n","   1.75005067e-02 -3.44303884e-02  3.76495719e-03 -6.25447333e-02\n","   4.94088791e-02  3.34168896e-02 -6.46147877e-02 -3.66886929e-02\n","  -5.02351783e-02 -8.46068840e-03 -8.72666296e-03 -4.48269211e-03\n","   2.29152385e-02 -5.03959022e-02 -2.16064434e-02  9.89727862e-03\n","   1.14491470e-02  4.26040851e-02 -5.79513721e-02  7.10346922e-03\n","  -1.59269776e-02 -2.44746394e-02 -2.79525556e-02  6.63634250e-03\n","   4.07587774e-02 -6.12530299e-02 -1.58712044e-02 -4.48136553e-02\n","  -6.32011965e-02 -5.98417483e-02 -6.26944080e-02 -6.21531978e-02\n","  -6.49571940e-02 -4.77006771e-02 -6.44478388e-03  1.19818924e-02\n","   3.59630026e-02  1.03433626e-02 -1.28233060e-02 -4.47486155e-02\n","  -1.28768776e-02 -1.31056597e-02 -1.28039503e-02 -4.92150672e-02\n","   3.63625557e-04 -2.88233627e-02 -6.77370206e-02 -9.43741354e-04\n","  -4.41355556e-02 -8.05429416e-04  1.24963000e-02  1.14342282e-02\n","  -4.43571247e-02 -4.72233370e-02 -5.36097623e-02 -5.25359018e-03\n","  -6.73572943e-02 -3.61211523e-02  4.77996934e-03 -2.83071827e-02\n","  -4.41841893e-02  3.47328149e-02  2.08213478e-02  6.31326996e-03\n","   1.99702196e-02 -4.15296433e-03 -6.44135987e-03 -3.52205485e-02\n","   9.34625883e-03 -6.19338006e-02 -4.59719412e-02 -4.61254716e-02\n","   1.20343799e-02 -5.98401465e-02 -1.81235764e-02 -4.03209813e-02\n","  -6.68178350e-02 -1.80482920e-02 -1.62219163e-02 -9.36911069e-03\n","  -5.95644750e-02 -6.57482296e-02 -4.25811633e-02 -4.28673886e-02\n","  -6.86221719e-02 -3.32974568e-02 -3.90349440e-02 -3.03354766e-02\n","  -2.75945347e-02 -1.58135444e-02 -5.46051823e-02  3.62375006e-02\n","  -1.64077338e-02 -2.81539876e-02 -5.31175137e-02 -2.07487121e-02\n","  -1.10382503e-02 -3.55628133e-02  5.15866354e-02 -6.45951629e-02\n","  -6.53586835e-02 -5.14774434e-02 -4.96147387e-02  1.95140690e-02\n","   6.42253365e-03  3.08058057e-02 -3.98363061e-02 -3.92420441e-02\n","  -4.91292365e-02 -8.18531844e-04 -1.80128701e-02  1.25617823e-02\n","  -6.39607832e-02 -7.71956891e-03 -5.78210540e-02 -6.65426701e-02\n","  -1.40746916e-02 -4.47316878e-02 -1.84389669e-03  1.76711529e-02\n","   5.25187366e-02 -4.45447378e-02 -4.49861772e-02 -3.27021331e-02\n","   2.07317434e-02 -3.21468748e-02  2.43276381e-03 -5.62764369e-02\n","   1.80409867e-02 -2.05694381e-02  4.40577753e-02  1.27921402e-02\n","  -3.27890404e-02 -4.96030338e-02 -6.60374202e-03  9.98678431e-03\n","  -4.42516431e-02  4.37076986e-02 -4.68783155e-02 -1.83882080e-02\n","  -4.96408679e-02 -3.97315100e-02 -5.51225878e-02  5.56388982e-02\n","  -3.30444239e-02 -5.61372489e-02 -4.78386618e-02 -1.15356697e-02\n","  -1.77313425e-02  1.78562440e-02 -6.00997098e-02 -4.29861620e-02\n","   4.34222072e-02 -5.42469211e-02 -3.70592512e-02  5.71789267e-03\n","   6.19765259e-02 -6.84540942e-02 -3.95829566e-02 -3.43120955e-02\n","  -6.38882220e-02 -1.71403550e-02 -2.27796510e-02 -5.39885163e-02\n","  -3.67116071e-02 -2.44412245e-03 -2.58599166e-02 -2.69275196e-02\n","  -6.12100679e-03 -2.75992937e-02 -6.06824905e-02  2.86420435e-02\n","  -3.71211302e-03 -4.79973666e-02 -2.72953548e-02 -1.57411620e-02\n","  -4.20200825e-03 -5.27623557e-02 -4.23479527e-02 -5.28243408e-02\n","   1.64412986e-02 -1.77333746e-02 -7.79292965e-03  2.51558628e-02\n","  -1.00870915e-02 -2.74149720e-02  4.28286381e-02  3.81740034e-02\n","  -2.86740512e-02 -4.82106057e-04  3.72500569e-02 -5.56382425e-02\n","  -5.93668595e-03 -5.40009625e-02  3.11122667e-02 -4.90091369e-02\n","   4.05580401e-02 -5.82085699e-02 -1.35729611e-02  3.36242886e-03\n","   3.35612036e-02 -2.70603895e-02 -4.43089381e-03 -6.17849529e-02\n","  -1.29680308e-02 -2.36707237e-02 -4.54604849e-02 -3.90718915e-02\n","  -4.84429970e-02 -4.21289494e-03  7.06333434e-03  1.46841081e-02\n","  -4.42114612e-03 -2.77990289e-02 -4.28136736e-02 -5.78616858e-02\n","   3.16888429e-02 -6.67357966e-02 -4.20282446e-02 -4.20838185e-02\n","   1.04076834e-02 -1.10017508e-02 -5.87451681e-02 -7.87611678e-03\n","  -3.35113034e-02 -1.66143104e-02 -4.83947806e-02 -1.52100092e-02\n","   2.17450727e-02  1.09169241e-02 -1.88933965e-02 -6.21402673e-02\n","  -4.91542816e-02  1.32418759e-02  4.11048159e-02  4.60882150e-02\n","   5.31011671e-02 -2.07015332e-02  3.84201668e-02 -1.18998559e-02\n","  -2.38589086e-02  1.89302061e-02 -3.59291509e-02  3.33064049e-02\n","  -6.33031651e-02  1.78659502e-02  2.18907669e-02 -4.30308804e-02\n","  -6.16740286e-02 -3.82871293e-02 -4.08039093e-02 -4.81129251e-02\n","  -4.24456932e-02 -4.59617116e-02 -1.32769980e-02 -3.15245613e-02\n","   2.53932457e-02 -4.25357260e-02  1.53949978e-02  3.14829126e-03\n","  -5.10172844e-02 -4.15738998e-03 -3.48822698e-02 -4.19747643e-02\n","   5.25464788e-02 -3.34876366e-02 -1.00780772e-02 -5.12382248e-03\n","   4.16240515e-03  8.45270115e-04  4.17529605e-03 -3.17049883e-02\n","  -8.23067501e-03 -3.72742005e-02 -1.56147415e-02 -1.92888435e-02\n","  -2.19631940e-02 -2.93814726e-02  1.81328934e-02 -4.21580039e-02\n","   4.58555035e-02 -2.65504047e-02 -1.08511578e-02  3.58534418e-02\n","   3.48922797e-02 -5.29149063e-02 -2.02202909e-02 -4.28987481e-02\n","  -1.51174683e-02 -3.31492089e-02 -3.79889132e-03 -1.90338586e-02\n","   3.89536247e-02 -2.24301666e-02 -3.15454253e-03 -1.82837378e-02\n","   4.93394881e-02 -3.45195457e-02  8.55283532e-03 -3.95641811e-02\n","   1.23109845e-02 -1.16149373e-02 -3.11416015e-02 -3.05825546e-02\n","  -5.95337003e-02 -5.11672162e-02  3.83612774e-02 -3.66385542e-02\n","  -6.58141673e-02 -5.65931685e-02 -2.44278312e-02 -1.22773629e-02\n","  -2.04281397e-02  2.18738522e-02  1.80172324e-02 -5.19400388e-02\n","  -5.74789718e-02 -2.90307961e-02 -6.18052706e-02 -3.00486758e-02\n","   1.43912872e-02 -6.75385119e-03 -4.26292047e-02 -4.66184132e-02\n","  -4.27307002e-02  3.00474707e-02 -5.40151447e-02 -1.21095162e-02\n","   1.17124394e-02  8.61700159e-03  8.32114927e-03  3.62269431e-02\n","  -4.31082547e-02 -4.48446609e-02 -1.56380720e-02 -6.79764375e-02\n","   1.14115030e-02 -4.93202312e-03 -2.57961894e-03 -1.51599916e-02\n","  -2.02644542e-02  2.21148203e-03 -6.68246970e-02 -2.69524492e-02\n","   1.15833548e-03 -4.00655344e-02 -1.28865317e-02 -2.45342907e-02\n","   6.51713787e-03 -8.09993595e-03 -2.02562679e-02 -2.30995361e-02\n","  -1.49497837e-02  4.54637371e-02  9.70757846e-03 -4.46383767e-02\n","   1.12311626e-02 -2.87881400e-02 -6.17218390e-02 -6.09731711e-02\n","  -6.54839203e-02 -1.30192442e-02 -6.35985583e-02 -1.53536275e-02\n","  -4.71060872e-02 -5.50869964e-02 -3.50413620e-02  4.47664689e-03\n","  -2.22488330e-03 -5.18177524e-02  9.83801088e-04 -1.59746017e-02\n","  -3.06758936e-02 -1.10467626e-02  1.42212305e-02 -6.59357533e-02\n","   3.56988050e-02 -6.17787838e-02 -5.12483567e-02 -7.76507705e-03\n","   1.95459723e-02  1.46789141e-02 -4.05520834e-02 -4.16425094e-02\n","  -2.90714502e-02 -4.51059677e-02  5.60347503e-03 -2.76053045e-02\n","  -1.87601969e-02 -5.73713658e-03 -2.34680884e-02 -9.66573134e-03\n","  -2.42818575e-02 -1.07973041e-02 -2.09799744e-02 -3.07571460e-02\n","  -4.23357375e-02  3.27984318e-02 -5.72499111e-02  2.95682009e-02\n","   7.99638964e-03 -2.09005941e-02 -2.05738638e-02 -5.24592847e-02\n","   1.46810170e-02 -4.11892012e-02 -4.05260101e-02 -2.37601306e-02\n","  -1.21445321e-02  1.65345445e-02 -3.78314070e-02 -3.19849551e-02\n","  -3.66653525e-03 -5.34206741e-02  3.98816243e-02 -4.44691516e-02\n","  -2.46632639e-02 -2.82914378e-02  2.10151188e-02  3.95686775e-02\n","  -2.70971395e-02  3.74639630e-02 -4.44802977e-02 -3.44384946e-02\n","  -2.62889359e-02 -4.04287055e-02  2.65936591e-02 -4.86004651e-02\n","  -3.66293862e-02  1.23548545e-02  5.09775663e-03  3.76187302e-02\n","   4.89659002e-03  9.14238393e-03 -4.45589833e-02  1.43728592e-02]]\n","--------------\n","embeddings4\n","[[-4.97885495e-02 -3.38698067e-02 -5.56728095e-02 -4.11474667e-02\n","   6.96313754e-03 -4.03976105e-02 -4.76951944e-03 -2.12800987e-02\n","  -6.11012019e-02  1.39927408e-02 -1.80224497e-02 -5.28114624e-02\n","  -3.29049006e-02 -1.51721407e-02  4.78463108e-03 -2.11835783e-02\n","  -2.07464769e-02 -4.33031805e-02 -1.76080782e-02 -5.77015914e-02\n","  -5.43669797e-02 -4.17429507e-02 -5.27659319e-02 -4.14895862e-02\n","  -5.88348582e-02 -2.04904843e-03 -3.59854400e-02 -3.29059176e-02\n","   9.45981592e-03 -8.26028269e-03 -6.31446987e-02 -2.33903956e-02\n","  -3.81065458e-02 -5.81418872e-02 -2.66057346e-02 -4.58577089e-02\n","  -3.64494361e-02 -9.08537675e-03  2.99682152e-02 -3.49967368e-02\n","  -7.81848002e-03  5.42252623e-02 -1.91977379e-04  9.89647489e-03\n","  -3.64799574e-02 -8.37358180e-03 -2.21418515e-02  2.16743238e-02\n","  -4.61373962e-02 -4.89573888e-02  1.55111467e-02 -5.99958673e-02\n","  -3.65251414e-02 -6.08441839e-03 -4.95022386e-02 -4.54390161e-02\n","  -1.82334352e-02 -4.90977988e-02 -5.10338023e-02 -1.22118974e-02\n","  -7.17385672e-03  4.64244820e-02  1.71236154e-02 -6.03799634e-02\n","   1.88992266e-02 -3.41076627e-02 -3.64180170e-02 -1.75270904e-02\n","  -4.21699397e-02 -3.00417300e-02 -1.71630010e-02  2.53294799e-02\n","  -5.20331925e-03  1.74616277e-02 -5.21779470e-02 -3.32685634e-02\n","  -2.84602139e-02 -6.08533695e-02 -5.04774675e-02  2.87055727e-02\n","  -1.83185581e-02 -3.14425044e-02  5.61792031e-02 -6.13015667e-02\n","  -6.02145493e-02 -4.03747372e-02 -2.62867124e-03 -5.48717156e-02\n","   4.22708839e-02 -2.94048544e-02  1.86704705e-03  2.22185198e-02\n","  -4.43052724e-02 -5.78736886e-02 -3.17290835e-02 -5.67528345e-02\n","  -5.73620833e-02  5.00985496e-02 -4.95999753e-02 -5.83087802e-02\n","  -4.68400978e-02 -5.53425178e-02 -7.76218669e-03  5.43324417e-03\n","  -2.45778747e-02 -5.91546595e-02 -3.73735689e-02 -8.25320557e-03\n","  -5.16817868e-02 -2.15709209e-02 -5.58209531e-02  5.30458502e-02\n","  -2.50978563e-02 -2.88826805e-02 -4.10584100e-02  4.18851450e-02\n","  -3.37077230e-02 -3.07248309e-02 -3.75158675e-02  1.09179690e-03\n","   1.94131630e-03 -2.01922208e-02 -2.00249534e-02  5.72568411e-03\n","   1.46668479e-02 -4.85771298e-02 -3.87736782e-02  3.57765593e-02\n","  -5.27035967e-02 -6.40959516e-02 -4.23320085e-02  2.09280215e-02\n","   8.92395401e-05 -5.43933511e-02  1.35133779e-02 -2.56832573e-03\n","  -3.88882421e-02 -5.99643737e-02 -3.95348575e-03 -3.82054690e-03\n","   1.17364097e-02 -4.27935980e-02 -5.13352677e-02 -4.63204924e-03\n","   1.62095539e-02 -1.48699265e-02 -3.51205356e-02  1.95197258e-02\n","  -5.60247414e-02 -3.61771719e-03 -5.97918220e-02  1.10580167e-02\n","   2.39168257e-02 -5.27790412e-02 -2.34520230e-02 -4.66633327e-02\n","   2.94013023e-02 -4.02192846e-02 -4.24587093e-02 -4.16656844e-02\n","  -3.28425653e-02  4.93932962e-02 -5.75624891e-02 -3.52687389e-02\n","  -4.81701717e-02  4.31510285e-02  9.49137565e-03 -6.63826824e-04\n","  -4.99229841e-02 -1.79540028e-03 -5.78752160e-02 -3.70023325e-02\n","  -4.60865721e-02 -6.06004409e-02 -4.65489142e-02 -3.14833932e-02\n","  -3.02837845e-02 -5.41168414e-02  3.82784195e-02 -5.01722656e-02\n","  -5.68829402e-02 -4.27640006e-02 -1.31044257e-02 -4.18538973e-02\n","  -6.12108260e-02  2.81192288e-02  2.41193678e-02 -4.20453772e-02\n","  -5.45385852e-02  3.49364765e-02 -4.10098433e-02  2.32913420e-02\n","  -3.55236977e-02 -3.46399061e-02 -4.92774434e-02 -6.03586575e-03\n","   6.13783067e-03  6.97365217e-03 -1.86317060e-02  5.00662364e-02\n","  -3.88909467e-02  9.08157788e-03 -5.13712168e-02 -4.33389544e-02\n","  -6.26792759e-02  4.38812934e-02 -5.05279228e-02 -4.37835641e-02\n","  -5.69483545e-03 -5.17607965e-02 -2.85986625e-02 -3.06973816e-03\n","   4.97662229e-04 -5.23146130e-02  4.68152985e-02  3.07584535e-02\n","  -6.48000315e-02 -5.03432266e-02  2.23890170e-02  7.47689977e-03\n","  -2.86439639e-02 -2.65093297e-02 -1.69195775e-02  5.06372713e-02\n","  -5.60577177e-02  3.03692231e-03 -3.97965498e-02  1.47624954e-03\n","   2.83621941e-02 -5.82403019e-02 -4.69047204e-02  3.96944210e-03\n","  -5.04432023e-02 -5.91306835e-02 -3.38372439e-02 -4.56573628e-02\n","  -4.16709855e-02 -3.11001930e-02 -2.17011608e-02 -5.07381968e-02\n","  -2.31108232e-03 -5.34837395e-02  3.60033438e-02 -2.88520399e-02\n","   5.02643436e-02 -5.88421524e-02  1.77213959e-02 -5.03082685e-02\n","  -5.86576089e-02  7.16052577e-03 -3.58043313e-02  5.04232419e-04\n","  -6.77462248e-03  1.40773160e-02 -7.45858997e-03 -1.51527096e-02\n","  -6.07057177e-02 -1.43806804e-02 -1.81651302e-02 -5.55482358e-02\n","   4.20346018e-03  1.87859200e-02 -4.21927087e-02 -4.78593186e-02\n","  -4.78920825e-02 -4.16122600e-02 -5.79427509e-03  6.01543533e-03\n","  -6.11000843e-02 -5.66163510e-02 -5.69303446e-02  2.81335087e-03\n","   6.95500057e-03 -5.83657436e-02 -1.22691114e-02 -4.16203924e-02\n","  -6.42433297e-03  2.60210242e-02  3.72668840e-02  3.20838615e-02\n","   1.18659961e-03 -2.86838021e-02 -5.44418134e-02  3.06741279e-02\n","  -5.25176972e-02 -3.63212153e-02 -5.13420627e-03 -6.07963204e-02\n","  -1.32799149e-02  3.65998549e-03  5.59908431e-03 -2.87926029e-02\n","  -1.95056926e-02  1.25378408e-02 -2.45596860e-02  6.33098511e-03\n","  -1.47513719e-02  4.95676249e-02 -2.70650499e-02 -5.39248958e-02\n","  -2.25146562e-02 -2.57002972e-02 -2.10160110e-02 -2.74762753e-02\n","  -2.82622315e-02 -4.37751524e-02  1.23516526e-02 -1.61601044e-02\n","   3.87360156e-03 -1.14182392e-02 -1.86256599e-02 -2.04564184e-02\n","  -1.15039004e-02 -4.64040712e-02 -5.00726849e-02  5.11671565e-02\n","  -3.90582271e-02 -2.93850172e-02  4.20506150e-02  2.88037434e-02\n","  -5.30836470e-02 -1.83581207e-02  1.81253534e-02 -3.84177268e-02\n","   1.29680661e-02 -5.94235817e-03  2.91574653e-02 -4.90992405e-02\n","  -5.94459474e-02  2.04798952e-02  5.09117134e-02 -7.75831798e-03\n","   4.03011627e-02 -5.94041571e-02  2.96966056e-03 -5.40871397e-02\n","   7.89997354e-03 -7.13109365e-03 -5.44762835e-02 -1.08182887e-02\n","  -6.19677603e-02  4.36585732e-02  1.92544423e-02 -2.98148338e-02\n","  -2.45979913e-02 -3.76308672e-02  6.15964120e-04 -2.39676889e-02\n","   6.32507680e-03 -2.02585980e-02 -2.38671359e-02  1.58751495e-02\n","  -1.69112831e-02 -5.37026934e-02  3.45720351e-02 -1.04291076e-02\n","   7.39191251e-04 -4.07892093e-02 -3.92501317e-02 -2.78667044e-02\n","  -8.51594936e-03 -3.98789765e-03 -6.32470697e-02 -4.42216098e-02\n","  -1.31204724e-02 -5.90629615e-02 -1.43685797e-02 -4.91667353e-02\n","   1.77006871e-02 -2.91979685e-02  3.85207869e-03  2.74792369e-02\n","  -1.82358623e-02  4.86526527e-02  7.39780441e-03 -5.08514643e-02\n","  -4.71879505e-02 -1.08554941e-02 -3.05745490e-02 -4.57282998e-02\n","   5.26989298e-03 -5.07337973e-02 -6.11059815e-02  3.16954330e-02\n","  -4.91765253e-02 -6.06914349e-02 -3.04878000e-02 -4.23086658e-02\n","  -9.81044839e-04 -3.19682173e-02 -5.82949165e-03 -1.87326539e-02\n","  -6.03353791e-02 -2.11837199e-02 -1.48140220e-02 -4.42865342e-02\n","  -1.34596671e-03 -1.48580940e-02  5.86302206e-03  8.37264489e-03\n","  -6.36650203e-03 -6.44322708e-02 -5.03987335e-02 -4.52791341e-02\n","   3.12409550e-02 -4.10870723e-02 -1.67989656e-02 -4.62443642e-02\n","  -5.78497760e-02  2.45877933e-02 -8.05856660e-03  3.14213671e-02\n","  -4.32788394e-02 -3.60111222e-02  6.45286357e-03 -1.71330255e-02\n","  -4.44231257e-02 -5.63553348e-02 -4.89633195e-02 -6.10404322e-03\n","  -4.69112545e-02 -4.16710563e-02 -5.07328846e-02 -3.54488380e-02\n","  -4.02210616e-02  9.47605353e-03 -5.51110655e-02 -2.21892092e-02\n","  -2.56962841e-03  3.07598040e-02 -6.08343445e-02  2.85811704e-02\n","  -4.95189615e-02 -3.06048431e-02  2.19735820e-02  5.14217168e-02\n","  -3.52489986e-02 -3.40141635e-03 -3.12203448e-02 -3.64248864e-02\n","  -2.28015929e-02 -1.68605838e-02  1.00838263e-02  1.52443275e-02\n","  -2.07028501e-02 -3.61249526e-03 -2.85152793e-02 -5.24119250e-02\n","  -4.11083736e-03  2.30520349e-02 -2.22215289e-03 -4.18089554e-02\n","   7.31402822e-03 -1.47345429e-03 -5.90135939e-02 -4.32412699e-02\n","  -2.79458822e-03 -4.62298170e-02  3.23292450e-03  9.95346252e-03\n","   2.78366506e-02 -2.09813341e-02  3.11144516e-02 -2.16291416e-02\n","   1.45238728e-05 -5.61325774e-02  4.15161289e-02  5.17496467e-02\n","   1.51937520e-02  3.60757746e-02 -5.36129288e-02 -2.26382818e-02\n","  -2.72270143e-02 -3.44801098e-02  3.03569604e-02 -2.70783007e-02\n","  -2.76574939e-02  3.01857907e-02 -5.79402521e-02 -1.01684919e-02\n","  -5.03160320e-02 -5.24175465e-02 -3.56832333e-02 -6.33376762e-02\n","   3.68094929e-02 -3.93113755e-02 -5.61262332e-02 -5.22207841e-02\n","   1.16200594e-03 -6.42847270e-02  6.14316761e-03 -2.06080042e-02\n","  -3.92088518e-02 -5.64089268e-02 -2.55094692e-02 -1.95392761e-02\n","  -3.96593735e-02 -1.81986000e-02 -2.75612064e-02  4.68704216e-02\n","  -8.99433065e-03 -5.01873679e-02 -1.72564778e-02  2.64538117e-02\n","  -3.43557000e-02 -4.27850932e-02 -5.53421564e-02  3.88631667e-03\n","  -5.58821335e-02 -3.89023162e-02 -7.60561181e-03 -4.86960299e-02\n","  -4.64235321e-02 -6.67504082e-03 -2.24828962e-02  9.63887607e-04\n","  -9.27556399e-03 -4.87681255e-02 -3.64151783e-02  1.54829081e-02\n","  -1.63027700e-02  2.77355853e-02  1.80592190e-03 -3.78363468e-02\n","  -6.19971417e-02 -6.08725995e-02  2.07500365e-02  3.68527211e-02\n","  -4.97372709e-02 -4.77972403e-02  2.15260796e-02 -5.27435318e-02\n","   3.29270982e-03 -2.08447855e-02 -2.88511869e-02 -3.51043716e-02\n","  -1.94232222e-02 -4.82617095e-02 -5.11623994e-02 -4.65528965e-02\n","  -3.05318031e-02 -5.73512837e-02  3.91766429e-02  1.53952288e-02\n","   5.31112682e-03 -5.09257577e-02  5.95162846e-02 -2.06055827e-02\n","   3.48230056e-03 -3.53046395e-02 -1.72073040e-02 -2.53125392e-02\n","  -5.11324927e-02  2.26705838e-02 -4.00122441e-02 -3.79687846e-02\n","   2.59704441e-02  2.57714489e-03 -2.20265742e-02 -3.59353572e-02\n","  -2.74576247e-02 -1.22162560e-02  5.48273209e-04 -5.87154962e-02\n","  -6.00104704e-02 -3.04538365e-02  6.73876563e-03  2.04654373e-02\n","  -1.75694227e-02 -1.39025142e-02  3.34628820e-02 -1.17446156e-02\n","  -4.04413864e-02  4.50952764e-04 -3.96440923e-02 -2.07587034e-02\n","  -3.68584432e-02 -5.05597629e-02 -4.77556251e-02  1.94041003e-02\n","  -2.58853287e-02 -5.27964160e-02  3.93597130e-03 -5.01895696e-02\n","  -1.48560423e-02 -3.52244638e-02 -4.08269688e-02 -9.48102679e-03\n","  -5.21330796e-02 -4.54658568e-02  4.07718197e-02 -9.98786185e-03\n","   1.75070986e-02 -1.40042575e-02 -4.08800542e-02 -2.14545857e-02\n","  -8.88764113e-03 -5.35273284e-04 -2.69676256e-03  8.05146899e-03\n","  -6.71073887e-03 -7.16809835e-03 -3.42009217e-02 -5.35352752e-02\n","  -2.91403532e-02 -1.19027961e-02  4.46636640e-02  1.79861579e-02\n","  -6.05858341e-02 -3.67836058e-02  6.71167625e-04  6.70204544e-03\n","   8.11110903e-03 -2.48146951e-02  7.81663973e-03 -4.33466360e-02\n","   4.31206711e-02 -4.78868112e-02 -1.91549510e-02  1.66564025e-02\n","  -4.42856997e-02 -9.39025450e-03 -3.33679542e-02  9.34312958e-03\n","   3.73425819e-02 -5.90666309e-02 -3.25398222e-02 -4.04780731e-02\n","  -5.07824831e-02 -2.28048284e-02  7.46418443e-03 -1.95917729e-02\n","   3.26731317e-02 -6.19077943e-02 -2.80978177e-02 -1.29841128e-03\n","  -5.61447181e-02 -4.63134050e-02  3.31096612e-02 -4.39533293e-02\n","   1.93075482e-02  1.40020037e-02 -5.08843511e-02  3.90050672e-02\n","  -3.16871554e-02 -7.93011766e-03 -1.85084902e-02 -2.12144926e-02\n","   3.16081420e-02  2.31311377e-02 -6.18668012e-02 -1.59334466e-02\n","   6.88154178e-05 -4.47186828e-02  4.38440889e-02 -6.04556166e-02\n","  -4.18292321e-02 -4.22884189e-02 -4.26427126e-02  1.55528458e-02\n","  -4.04137000e-02  3.30538787e-02  3.82237062e-02 -4.21844907e-02\n","  -4.73140031e-02 -5.85814789e-02  2.55771093e-02 -3.37574668e-02\n","  -7.24530255e-04 -5.02450541e-02  5.52650774e-03 -4.52687740e-02\n","  -2.45824251e-02 -1.95669383e-02  2.41759121e-02 -4.40597236e-02\n","   8.27010605e-04 -2.32897736e-02 -2.48973519e-02 -2.03278903e-02\n","   9.74817108e-03 -4.57353070e-02  3.49612348e-02 -8.06371495e-03\n","  -1.96049791e-02  2.34453939e-02 -6.35232851e-02 -3.15964920e-03\n","  -4.52744476e-02  2.31816452e-02 -1.23818573e-02 -4.24197130e-02\n","  -5.90133704e-02  5.06107174e-02 -3.64007317e-02 -1.77509431e-02\n","  -4.16077441e-03 -4.87626642e-02 -5.14556728e-02 -2.52677947e-02\n","  -4.37835678e-02  5.11069596e-03 -1.02068763e-02 -5.96549734e-02\n","   3.01313354e-03 -5.68632409e-02 -1.45048881e-02 -5.63666262e-02\n","   4.95117903e-02 -3.99129844e-04 -5.42622991e-02 -4.04777713e-02\n","  -3.31597477e-02 -2.74950452e-02 -1.55850984e-02 -5.60215069e-03\n","   5.04566077e-03 -3.05827390e-02 -2.53779311e-02 -1.13232210e-02\n","   3.06888781e-02 -5.67103364e-03  4.49659815e-03 -5.27441800e-02\n","   4.90249246e-02 -9.73304734e-03 -5.78533709e-02 -6.95523433e-03\n","  -5.62369935e-02 -4.17069979e-02 -1.90849882e-04 -4.96374927e-02\n","  -6.14957064e-02  1.79984402e-02 -5.66425174e-02 -5.64625412e-02\n","  -4.64532785e-02 -6.12469390e-02 -4.87997495e-02 -4.40019667e-02\n","   8.70759133e-04  2.11960338e-02 -5.90859167e-03 -1.06363948e-02\n","  -7.44809536e-03 -6.12057485e-02  1.84588060e-02 -9.68598481e-03\n","  -5.85083701e-02 -4.14426103e-02 -8.06490704e-03  3.24534401e-02\n","  -2.36361083e-02 -1.09566618e-02 -5.19896038e-02  5.19982830e-04\n","  -2.89882123e-02 -3.73451486e-02 -6.74957270e-03  1.68130791e-04\n","   1.41793722e-02 -5.56947738e-02 -5.09525649e-02  3.42374109e-02\n","   2.79987715e-02  2.37941481e-02 -2.71094926e-02  7.11742602e-03]]\n","--------------\n","tensor([[0.9538]])\n","tensor([[0.8226]])\n","tensor([[0.4237]])\n"]}],"source":["from sentence_transformers import SentenceTransformer,util\n","sentences1 = [\"This is an example sentence\"]\n","sentences2 = [\"این یک جمله مثال است\"]\n","sentences3 = [\"این مثال است\"]\n","sentences4 = [\"امروز دوشنبه است\"]\n","\n","model = SentenceTransformer('sentence-transformers/LaBSE')\n","embeddings1 = model.encode(sentences1)\n","print(\"embeddings1\")\n","print(embeddings1)\n","print(\"--------------\")\n","print(\"embeddings2\")\n","embeddings2 = model.encode(sentences2)\n","print(embeddings2)\n","print(\"--------------\")\n","embeddings3 = model.encode(sentences3)\n","print(\"embeddings3\")\n","print(embeddings3)\n","print(\"--------------\")\n","print(\"embeddings4\")\n","embeddings4 = model.encode(sentences4)\n","print(embeddings4)\n","print(\"--------------\")\n","print(util.cos_sim(embeddings1,embeddings2))\n","print(util.cos_sim(embeddings1,embeddings3))\n","print(util.cos_sim(embeddings1,embeddings4))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gn-DrsjZI81"},"outputs":[],"source":["def align(sentences1,sentences2,window_size):\n","    scores = {}\n","    resutls = []\n","\n","    for i in range(0,len(sentences1)):\n","        for j in range(i,len(sentences1)):\n","            for k in range(0,len(sentences2)):\n","                for l in range(k,len(sentences2)):\n","                    sents1 = \"\"\n","                    sents2 = \"\"\n","                    for t in range(i,j+1):\n","                        sents1 = sents1 + \" \" +  sentences1[t] \n","                    for t in range(k,l+1):\n","                        sents2 = sents2 + \" \" +  sentences2[t] \n","                    \n","                    if i not in scores:\n","                        scores[i]={}\n","                    if j not in scores[i]:\n","                        scores[i][j] = {}\n","                    if k not in scores[i][j]:\n","                        scores[i][j][k] = {}\n","                    # if l not in scores[i][j][k]:\n","                    #     scores[i][j][k][l] = {}\n","                    if (abs(i-j) < window_size) and (abs(k-l) < window_size):\n","                      embeddings1 = model.encode([sents1], convert_to_tensor=True)\n","                      embeddings2 = model.encode([sents2], convert_to_tensor=True)\n","                      scores[i][j][k][l] =  [util.cos_sim(embeddings1,embeddings2).cpu().numpy()[0][0],\"active\"]\n","                      #print(\"{}-{}-{}-{}\".format(i,j,k,l))\n","                    else:\n","                      scores[i][j][k][l] = [0, \"deactivate\"]\n","                    print(\"{}-{}-{}-{}-{}\".format(i,j,k,l,scores[i][j][k][l]))\n","\n","            \n","    # print(scores)\n","\n","\n","    counter = 0\n","\n","    while True:\n","    \n","        t = 0 \n","        for i in range(0,len(sentences1)):\n","            for j in range(i,len(sentences1)):\n","                for k in range(0,len(sentences2)):\n","                    for l in range(k,len(sentences2)):\n","                        if t==0 and scores[i][j][k][l][1] == \"active\":\n","                            a = i\n","                            b = j\n","                            c = k\n","                            d = l\n","                            t = 1\n","                            #print(\"x-{}-{}-{}-{}\".format(a,b,c,d))\n","\n","                        #print(scores[i][j][k][l][0])\n","                        if t == 1:\n","                            if scores[i][j][k][l][0] > scores[a][b][c][d][0] and scores[i][j][k][l][1] == \"active\":\n","                                a = i\n","                                b = j\n","                                c = k\n","                                d = l\n","\n","        \n","        if t ==0:\n","            for i in range(0,len(sentences1)):\n","                for j in range(i,len(sentences1)):\n","                    for k in range(0,len(sentences2)):\n","                        for l in range(k,len(sentences2)):\n","                            if scores[i][j][k][l][1] == \"selected\":\n","                                resutls.append([i,j,k,l,scores[i][j][k][l][0]])\n","\n","            return resutls,scores\n","\n","        print('selected-----')\n","        print(\"{}-{}-{}-{}\".format(a,b,c,d))\n","\n","        scores[a][b][c][d][1] = \"selected\"\n","        #resutls.append([a,b,c,d])\n","\n","        #print(\"---\")\n","\n","\n","        for i in range(0,len(sentences1)):\n","            for j in range(i,len(sentences1)):\n","                for k in range(0,len(sentences2)):\n","                    for l in range(k,len(sentences2)):\n","                        if (i >= a and i <= b) or\\\n","                            (j >= a and j <= b) or\\\n","                            (k >= c and k <= d) or\\\n","                            (l >= c and l <= d) or\\\n","                            (a >= i and a <= j) or\\\n","                            (b >= i and b <= j) or\\\n","                            (c >= k and c <= l) or\\\n","                            (d >= k and d <= l) or\\\n","                            (i<=a and l>=d) or\\\n","                            (i>=b and l<=c):\n","                                if scores[i][j][k][l][1] == \"active\" :\n","                                    scores[i][j][k][l][1] = \"deactivate\"\n","                                    print(\"deactive----\")\n","                                    print(\"{}-{}-{}-{}\".format(i,j,k,l))\n","\n","                        \n","        #print(scores)\n","        #\n","        #if counter == 10 : break\n","        \n","        \n","        \n","\n","        counter += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJw5-S1wffR3"},"outputs":[],"source":["# !pip install ipython-autotime\n","# text1 = open(\"/content/drive/MyDrive/farsi.txt\",\"r\").read()\n","# text2 = open(\"/content/drive/MyDrive/english.txt\",\"r\").read()\n","# sentences1 = text1.split(\"\\n\")\n","# sentences2 = text2.split(\"\\n\")\n","# %load_ext autotime\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RuRW_wM9-txW"},"outputs":[],"source":["sentences1 = \"I live in a house near the mountains. I have two brothers and one sister, and I was born last. My father teaches mathematics, and my mother is a nurse at a big hospital.\".split('.')\n","sentences2 = \"من در خانه ای نزدیک کوه زندگی می کنم. دو برادر و یک خواهر دارم و آخرین به دنیا آمدم. پدرم ریاضی تدریس می کند و مادرم پرستار یک بیمارستان بزرگ است. برادران من بسیار باهوش هستند و در مدرسه سخت کار می کنند. خواهر من دختر عصبی است اما بسیار مهربان است. \".split(\".\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57,"status":"ok","timestamp":1680538400238,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":-210},"id":"lyShrnw7tYjB","outputId":"2569d05c-e6e8-4334-b6ec-3314287199c2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":8}],"source":["len(sentences1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1680538400239,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":-210},"id":"zcyoINDx1CRC","outputId":"301d949f-c757-474a-d4e6-8a559ebd1d20"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{},"execution_count":9}],"source":["len(sentences2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n955htDn0qdg","outputId":"43e08af3-889c-4ba0-9ee7-7258f0a40457","executionInfo":{"status":"ok","timestamp":1680538401768,"user_tz":-210,"elapsed":1572,"user":{"displayName":"Nikki M","userId":"12345895666301116120"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0-0-0-0-[0.94452703, 'active']\n","0-0-0-1-[0.71031094, 'active']\n","0-0-0-2-[0, 'deactivate']\n","0-0-0-3-[0, 'deactivate']\n","0-0-0-4-[0, 'deactivate']\n","0-0-0-5-[0, 'deactivate']\n","0-0-1-1-[0.43329367, 'active']\n","0-0-1-2-[0.40223244, 'active']\n","0-0-1-3-[0, 'deactivate']\n","0-0-1-4-[0, 'deactivate']\n","0-0-1-5-[0, 'deactivate']\n","0-0-2-2-[0.3748255, 'active']\n","0-0-2-3-[0.38048577, 'active']\n","0-0-2-4-[0, 'deactivate']\n","0-0-2-5-[0, 'deactivate']\n","0-0-3-3-[0.36309165, 'active']\n","0-0-3-4-[0.3298859, 'active']\n","0-0-3-5-[0, 'deactivate']\n","0-0-4-4-[0.3536355, 'active']\n","0-0-4-5-[0.3536355, 'active']\n","0-0-5-5-[0.38502705, 'active']\n","0-1-0-0-[0.6438006, 'active']\n","0-1-0-1-[0.9189936, 'active']\n","0-1-0-2-[0, 'deactivate']\n","0-1-0-3-[0, 'deactivate']\n","0-1-0-4-[0, 'deactivate']\n","0-1-0-5-[0, 'deactivate']\n","0-1-1-1-[0.71511877, 'active']\n","0-1-1-2-[0.6496388, 'active']\n","0-1-1-3-[0, 'deactivate']\n","0-1-1-4-[0, 'deactivate']\n","0-1-1-5-[0, 'deactivate']\n","0-1-2-2-[0.3636382, 'active']\n","0-1-2-3-[0.40298277, 'active']\n","0-1-2-4-[0, 'deactivate']\n","0-1-2-5-[0, 'deactivate']\n","0-1-3-3-[0.36011314, 'active']\n","0-1-3-4-[0.41692817, 'active']\n","0-1-3-5-[0, 'deactivate']\n","0-1-4-4-[0.39275873, 'active']\n","0-1-4-5-[0.39275873, 'active']\n","0-1-5-5-[0.27991334, 'active']\n","0-2-0-0-[0, 'deactivate']\n","0-2-0-1-[0, 'deactivate']\n","0-2-0-2-[0, 'deactivate']\n","0-2-0-3-[0, 'deactivate']\n","0-2-0-4-[0, 'deactivate']\n","0-2-0-5-[0, 'deactivate']\n","0-2-1-1-[0, 'deactivate']\n","0-2-1-2-[0, 'deactivate']\n","0-2-1-3-[0, 'deactivate']\n","0-2-1-4-[0, 'deactivate']\n","0-2-1-5-[0, 'deactivate']\n","0-2-2-2-[0, 'deactivate']\n","0-2-2-3-[0, 'deactivate']\n","0-2-2-4-[0, 'deactivate']\n","0-2-2-5-[0, 'deactivate']\n","0-2-3-3-[0, 'deactivate']\n","0-2-3-4-[0, 'deactivate']\n","0-2-3-5-[0, 'deactivate']\n","0-2-4-4-[0, 'deactivate']\n","0-2-4-5-[0, 'deactivate']\n","0-2-5-5-[0, 'deactivate']\n","0-3-0-0-[0, 'deactivate']\n","0-3-0-1-[0, 'deactivate']\n","0-3-0-2-[0, 'deactivate']\n","0-3-0-3-[0, 'deactivate']\n","0-3-0-4-[0, 'deactivate']\n","0-3-0-5-[0, 'deactivate']\n","0-3-1-1-[0, 'deactivate']\n","0-3-1-2-[0, 'deactivate']\n","0-3-1-3-[0, 'deactivate']\n","0-3-1-4-[0, 'deactivate']\n","0-3-1-5-[0, 'deactivate']\n","0-3-2-2-[0, 'deactivate']\n","0-3-2-3-[0, 'deactivate']\n","0-3-2-4-[0, 'deactivate']\n","0-3-2-5-[0, 'deactivate']\n","0-3-3-3-[0, 'deactivate']\n","0-3-3-4-[0, 'deactivate']\n","0-3-3-5-[0, 'deactivate']\n","0-3-4-4-[0, 'deactivate']\n","0-3-4-5-[0, 'deactivate']\n","0-3-5-5-[0, 'deactivate']\n","1-1-0-0-[0.3876823, 'active']\n","1-1-0-1-[0.7722023, 'active']\n","1-1-0-2-[0, 'deactivate']\n","1-1-0-3-[0, 'deactivate']\n","1-1-0-4-[0, 'deactivate']\n","1-1-0-5-[0, 'deactivate']\n","1-1-1-1-[0.93050504, 'active']\n","1-1-1-2-[0.7538279, 'active']\n","1-1-1-3-[0, 'deactivate']\n","1-1-1-4-[0, 'deactivate']\n","1-1-1-5-[0, 'deactivate']\n","1-1-2-2-[0.4145577, 'active']\n","1-1-2-3-[0.44670564, 'active']\n","1-1-2-4-[0, 'deactivate']\n","1-1-2-5-[0, 'deactivate']\n","1-1-3-3-[0.459153, 'active']\n","1-1-3-4-[0.48468608, 'active']\n","1-1-3-5-[0, 'deactivate']\n","1-1-4-4-[0.4885193, 'active']\n","1-1-4-5-[0.4885193, 'active']\n","1-1-5-5-[0.28540516, 'active']\n","1-2-0-0-[0.32040483, 'active']\n","1-2-0-1-[0.66242534, 'active']\n","1-2-0-2-[0, 'deactivate']\n","1-2-0-3-[0, 'deactivate']\n","1-2-0-4-[0, 'deactivate']\n","1-2-0-5-[0, 'deactivate']\n","1-2-1-1-[0.7005164, 'active']\n","1-2-1-2-[0.8994503, 'active']\n","1-2-1-3-[0, 'deactivate']\n","1-2-1-4-[0, 'deactivate']\n","1-2-1-5-[0, 'deactivate']\n","1-2-2-2-[0.69106734, 'active']\n","1-2-2-3-[0.7169312, 'active']\n","1-2-2-4-[0, 'deactivate']\n","1-2-2-5-[0, 'deactivate']\n","1-2-3-3-[0.51354814, 'active']\n","1-2-3-4-[0.5427751, 'active']\n","1-2-3-5-[0, 'deactivate']\n","1-2-4-4-[0.41934532, 'active']\n","1-2-4-5-[0.41934532, 'active']\n","1-2-5-5-[0.26117805, 'active']\n","1-3-0-0-[0, 'deactivate']\n","1-3-0-1-[0, 'deactivate']\n","1-3-0-2-[0, 'deactivate']\n","1-3-0-3-[0, 'deactivate']\n","1-3-0-4-[0, 'deactivate']\n","1-3-0-5-[0, 'deactivate']\n","1-3-1-1-[0, 'deactivate']\n","1-3-1-2-[0, 'deactivate']\n","1-3-1-3-[0, 'deactivate']\n","1-3-1-4-[0, 'deactivate']\n","1-3-1-5-[0, 'deactivate']\n","1-3-2-2-[0, 'deactivate']\n","1-3-2-3-[0, 'deactivate']\n","1-3-2-4-[0, 'deactivate']\n","1-3-2-5-[0, 'deactivate']\n","1-3-3-3-[0, 'deactivate']\n","1-3-3-4-[0, 'deactivate']\n","1-3-3-5-[0, 'deactivate']\n","1-3-4-4-[0, 'deactivate']\n","1-3-4-5-[0, 'deactivate']\n","1-3-5-5-[0, 'deactivate']\n","2-2-0-0-[0.32134253, 'active']\n","2-2-0-1-[0.38924718, 'active']\n","2-2-0-2-[0, 'deactivate']\n","2-2-0-3-[0, 'deactivate']\n","2-2-0-4-[0, 'deactivate']\n","2-2-0-5-[0, 'deactivate']\n","2-2-1-1-[0.37718055, 'active']\n","2-2-1-2-[0.7389172, 'active']\n","2-2-1-3-[0, 'deactivate']\n","2-2-1-4-[0, 'deactivate']\n","2-2-1-5-[0, 'deactivate']\n","2-2-2-2-[0.90420246, 'active']\n","2-2-2-3-[0.84487236, 'active']\n","2-2-2-4-[0, 'deactivate']\n","2-2-2-5-[0, 'deactivate']\n","2-2-3-3-[0.5492121, 'active']\n","2-2-3-4-[0.5035848, 'active']\n","2-2-3-5-[0, 'deactivate']\n","2-2-4-4-[0.3950955, 'active']\n","2-2-4-5-[0.3950955, 'active']\n","2-2-5-5-[0.2665187, 'active']\n","2-3-0-0-[0.32134253, 'active']\n","2-3-0-1-[0.38924718, 'active']\n","2-3-0-2-[0, 'deactivate']\n","2-3-0-3-[0, 'deactivate']\n","2-3-0-4-[0, 'deactivate']\n","2-3-0-5-[0, 'deactivate']\n","2-3-1-1-[0.37718055, 'active']\n","2-3-1-2-[0.7389172, 'active']\n","2-3-1-3-[0, 'deactivate']\n","2-3-1-4-[0, 'deactivate']\n","2-3-1-5-[0, 'deactivate']\n","2-3-2-2-[0.90420246, 'active']\n","2-3-2-3-[0.84487236, 'active']\n","2-3-2-4-[0, 'deactivate']\n","2-3-2-5-[0, 'deactivate']\n","2-3-3-3-[0.5492121, 'active']\n","2-3-3-4-[0.5035848, 'active']\n","2-3-3-5-[0, 'deactivate']\n","2-3-4-4-[0.3950955, 'active']\n","2-3-4-5-[0.3950955, 'active']\n","2-3-5-5-[0.2665187, 'active']\n","3-3-0-0-[0.37391162, 'active']\n","3-3-0-1-[0.3354972, 'active']\n","3-3-0-2-[0, 'deactivate']\n","3-3-0-3-[0, 'deactivate']\n","3-3-0-4-[0, 'deactivate']\n","3-3-0-5-[0, 'deactivate']\n","3-3-1-1-[0.33261284, 'active']\n","3-3-1-2-[0.31341228, 'active']\n","3-3-1-3-[0, 'deactivate']\n","3-3-1-4-[0, 'deactivate']\n","3-3-1-5-[0, 'deactivate']\n","3-3-2-2-[0.3066844, 'active']\n","3-3-2-3-[0.3098788, 'active']\n","3-3-2-4-[0, 'deactivate']\n","3-3-2-5-[0, 'deactivate']\n","3-3-3-3-[0.30127087, 'active']\n","3-3-3-4-[0.28910172, 'active']\n","3-3-3-5-[0, 'deactivate']\n","3-3-4-4-[0.27195278, 'active']\n","3-3-4-5-[0.27195278, 'active']\n","3-3-5-5-[1.0, 'active']\n","selected-----\n","3-3-5-5\n","deactive----\n","0-0-4-5\n","deactive----\n","0-0-5-5\n","deactive----\n","0-1-4-5\n","deactive----\n","0-1-5-5\n","deactive----\n","1-1-4-5\n","deactive----\n","1-1-5-5\n","deactive----\n","1-2-4-5\n","deactive----\n","1-2-5-5\n","deactive----\n","2-2-4-5\n","deactive----\n","2-2-5-5\n","deactive----\n","2-3-0-0\n","deactive----\n","2-3-0-1\n","deactive----\n","2-3-1-1\n","deactive----\n","2-3-1-2\n","deactive----\n","2-3-2-2\n","deactive----\n","2-3-2-3\n","deactive----\n","2-3-3-3\n","deactive----\n","2-3-3-4\n","deactive----\n","2-3-4-4\n","deactive----\n","2-3-4-5\n","deactive----\n","2-3-5-5\n","deactive----\n","3-3-0-0\n","deactive----\n","3-3-0-1\n","deactive----\n","3-3-1-1\n","deactive----\n","3-3-1-2\n","deactive----\n","3-3-2-2\n","deactive----\n","3-3-2-3\n","deactive----\n","3-3-3-3\n","deactive----\n","3-3-3-4\n","deactive----\n","3-3-4-4\n","deactive----\n","3-3-4-5\n","selected-----\n","0-0-0-0\n","deactive----\n","0-0-0-1\n","deactive----\n","0-0-1-1\n","deactive----\n","0-0-1-2\n","deactive----\n","0-0-2-2\n","deactive----\n","0-0-2-3\n","deactive----\n","0-0-3-3\n","deactive----\n","0-0-3-4\n","deactive----\n","0-0-4-4\n","deactive----\n","0-1-0-0\n","deactive----\n","0-1-0-1\n","deactive----\n","0-1-1-1\n","deactive----\n","0-1-1-2\n","deactive----\n","0-1-2-2\n","deactive----\n","0-1-2-3\n","deactive----\n","0-1-3-3\n","deactive----\n","0-1-3-4\n","deactive----\n","0-1-4-4\n","deactive----\n","1-1-0-0\n","deactive----\n","1-1-0-1\n","deactive----\n","1-2-0-0\n","deactive----\n","1-2-0-1\n","deactive----\n","2-2-0-0\n","deactive----\n","2-2-0-1\n","selected-----\n","1-1-1-1\n","deactive----\n","1-1-1-2\n","deactive----\n","1-1-2-2\n","deactive----\n","1-1-2-3\n","deactive----\n","1-1-3-3\n","deactive----\n","1-1-3-4\n","deactive----\n","1-1-4-4\n","deactive----\n","1-2-1-1\n","deactive----\n","1-2-1-2\n","deactive----\n","1-2-2-2\n","deactive----\n","1-2-2-3\n","deactive----\n","1-2-3-3\n","deactive----\n","1-2-3-4\n","deactive----\n","1-2-4-4\n","deactive----\n","2-2-1-1\n","deactive----\n","2-2-1-2\n","selected-----\n","2-2-2-2\n","deactive----\n","2-2-2-3\n","deactive----\n","2-2-3-3\n","deactive----\n","2-2-3-4\n","deactive----\n","2-2-4-4\n"]}],"source":["x,y = align(sentences1,sentences2,2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":631,"status":"ok","timestamp":1680538402395,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":-210},"id":"3Z5bEFN-Vhak","outputId":"3a73533d-0683-46e5-cb20-b21855dafca5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0, 0, 0, 0, 0.94452703],\n"," [1, 1, 1, 1, 0.93050504],\n"," [2, 2, 2, 2, 0.90420246],\n"," [3, 3, 5, 5, 1.0]]"]},"metadata":{},"execution_count":11}],"source":["x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1680538402396,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":-210},"id":"K2KC9suXVn-S","outputId":"664e1670-2798-416d-c675-5f57a3dbe2ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I live in a house near the mountains',\n"," ' I have two brothers and one sister, and I was born last',\n"," ' My father teaches mathematics, and my mother is a nurse at a big hospital',\n"," '']"]},"metadata":{},"execution_count":12}],"source":["sentences1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1680538402397,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":-210},"id":"vL7m-qRwXdVf","outputId":"8b21fa3d-bce7-4b18-90c1-55e670405044"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['من در خانه ای نزدیک کوه زندگی می کنم',\n"," ' دو برادر و یک خواهر دارم و آخرین به دنیا آمدم',\n"," ' پدرم ریاضی تدریس می کند و مادرم پرستار یک بیمارستان بزرگ است',\n"," ' برادران من بسیار باهوش هستند و در مدرسه سخت کار می کنند',\n"," ' خواهر من دختر عصبی است اما بسیار مهربان است',\n"," ' ']"]},"metadata":{},"execution_count":13}],"source":["sentences2"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1680538402398,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":-210},"id":"10J9aFpPNPin","colab":{"base_uri":"https://localhost:8080/"},"outputId":"385d10a0-59c3-4f5b-8dbb-8712c6e94248"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0,0]:[0,0],0.94452703\n","\n","[1,1]:[1,1],0.93050504\n","\n","[2,2]:[2,2],0.90420246\n","\n","[3,3]:[5,5],1.0\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":14}],"source":["text3 = open(\"/content/drive/MyDrive/res3.txt\",\"w\")\n","text = \"\"\n","for a in x:\n","  t = \"[{},{}]:[{},{}],{}\\n\".format(str(a[0]),str(a[1]),str(a[2]),str(a[3]),str(a[4]))\n","  print(t)\n","  text += t \n","text3.write(t)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":501,"status":"ok","timestamp":1680538402890,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":-210},"id":"wvR6sb2g0Foy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"33ddfbcf-f80e-43ef-97c3-c3edc050be4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0]:[0]\n","\n","[1]:[1]\n","\n","[2]:[2]\n","\n","[3]:[5]\n","\n","[0]:[0]\n","[1]:[1]\n","[2]:[2]\n","[3]:[5]\n","\n"]}],"source":["text3 = open(\"/content/drive/MyDrive/res5.txt\",\"w\")\n","text = \"\"\n","for a in x:\n","  a1 = str(a[0])\n","  a2 = str(a[2])\n","  for i in range(a[0]+1,a[1]+1):\n","    a1 = a1 +  \",\" + str(i)\n","\n","  for i in range(a[2]+1,a[3]+1):\n","    a2 = a2 +  \",\" + str(i)\n","\n","  \n","\n","  t = \"[{}]:[{}]\\n\".format(a1,a2)\n","  print(t)\n","  text += t \n","print(text)\n","text3.write(text)\n","text3.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ba-ypl_Lw5Tu"},"outputs":[],"source":["# open(\"/content/drive/MyDrive/res.txt\",\"w\").write(\"dfsdf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1s6u-cGWLteL"},"outputs":[],"source":["import numpy as np\n","from collections import defaultdict\n","from ast import literal_eval\n","\n","\n","\n","def _precision(goldalign, testalign):\n","    \"\"\"\n","    Computes tpstrict, fpstrict, tplax, fplax for gold/test alignments\n","    \"\"\"\n","    tpstrict = 0  # true positive strict counter\n","    tplax = 0     # true positive lax counter\n","    fpstrict = 0  # false positive strict counter\n","    fplax = 0     # false positive lax counter\n","\n","    # convert to sets, remove alignments empty on both sides\n","    print(testalign)\n","    \n","    testalign = set([(tuple(x), tuple(y)) for x, y in testalign if len(x) or len(y)])\n","    goldalign = set([(tuple(x), tuple(y)) for x, y in goldalign if len(x) or len(y)])\n","\n","    # mappings from source test sentence idxs to\n","    #    target gold sentence idxs for which the source test sentence \n","    #    was found in corresponding source gold alignment\n","    src_id_to_gold_tgt_ids = defaultdict(set)\n","    for gold_src, gold_tgt in goldalign:\n","        for gold_src_id in gold_src:\n","            for gold_tgt_id in gold_tgt:\n","                src_id_to_gold_tgt_ids[gold_src_id].add(gold_tgt_id)\n","\n","    for (test_src, test_target) in testalign:\n","        if (test_src, test_target) == ((), ()):\n","            continue\n","        if (test_src, test_target) in goldalign:\n","            # strict match\n","            tpstrict += 1\n","            tplax += 1\n","        else:\n","            # For anything with partial gold/test overlap on the source,\n","            #   see if there is also partial overlap on the gold/test target\n","            # If so, its a lax match\n","            target_ids = set()\n","            for src_test_id in test_src:\n","                for tgt_id in src_id_to_gold_tgt_ids[src_test_id]:\n","                    target_ids.add(tgt_id)\n","            if set(test_target).intersection(target_ids):\n","                fpstrict += 1\n","                tplax += 1\n","            else:\n","                fpstrict += 1\n","                fplax += 1\n","\n","    return np.array([tpstrict, fpstrict, tplax, fplax], dtype=np.int32)\n","\n","\n","def score_multiple(gold_list, test_list, value_for_div_by_0=0.0):\n","    # accumulate counts for all gold/test files\n","    pcounts = np.array([0, 0, 0, 0], dtype=np.int32)\n","    rcounts = np.array([0, 0, 0, 0], dtype=np.int32)\n","    for goldalign, testalign in zip(gold_list, test_list):\n","        pcounts += _precision(goldalign=goldalign, testalign=testalign)\n","        # recall is precision with no insertion/deletion and swap args\n","        test_no_del = [(x, y) for x, y in testalign if len(x) and len(y)]\n","        gold_no_del = [(x, y) for x, y in goldalign if len(x) and len(y)]\n","        rcounts += _precision(goldalign=test_no_del, testalign=gold_no_del)\n","\n","    # Compute results\n","    # pcounts: tpstrict,fnstrict,tplax,fnlax\n","    # rcounts: tpstrict,fpstrict,tplax,fplax\n","\n","    if pcounts[0] + pcounts[1] == 0:\n","        pstrict = value_for_div_by_0\n","    else:\n","        pstrict = pcounts[0] / float(pcounts[0] + pcounts[1])\n","\n","    if pcounts[2] + pcounts[3] == 0:\n","        plax = value_for_div_by_0\n","    else:\n","        plax = pcounts[2] / float(pcounts[2] + pcounts[3])\n","\n","    if rcounts[0] + rcounts[1] == 0:\n","        rstrict = value_for_div_by_0\n","    else:\n","        rstrict = rcounts[0] / float(rcounts[0] + rcounts[1])\n","\n","    if rcounts[2] + rcounts[3] == 0:\n","        rlax = value_for_div_by_0\n","    else:\n","        rlax = rcounts[2] / float(rcounts[2] + rcounts[3])\n","\n","    if (pstrict + rstrict) == 0:\n","        fstrict = value_for_div_by_0\n","    else:\n","        fstrict = 2 * (pstrict * rstrict) / (pstrict + rstrict)\n","\n","    if (plax + rlax) == 0:\n","        flax = value_for_div_by_0\n","    else:\n","        flax = 2 * (plax * rlax) / (plax + rlax)\n","\n","    result = dict(recall_strict=rstrict,\n","                  recall_lax=rlax,\n","                  precision_strict=pstrict,\n","                  precision_lax=plax,\n","                  f1_strict=fstrict,\n","                  f1_lax=flax)\n","\n","    return result\n","\n","def read_alignments(infile):\n","    alignments = []\n","    for line in infile:\n","            fields = [x.strip() for x in line.split(':') if len(x.strip())]\n","            if len(fields) < 2:\n","                raise Exception('Got line \"%s\", which does not have at least two \":\" separated fields' % line.strip())\n","            try:\n","                src = literal_eval(fields[0])\n","                tgt = literal_eval(fields[1])\n","            except:\n","                raise Exception('Failed to parse line \"%s\"' % line.strip())\n","            alignments.append((src, tgt))\n","\n","    # I know bluealign files have a few entries entries missing,\n","    #   but I don't fix them in order to be consistent previous reported scores\n","    return alignments\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vfGmnyXSUAW7"},"outputs":[],"source":["# golds = open(\"/content/drive/MyDrive/gold.txt\",\"r\").read().splitlines()\n","\n","# aligns = open(\"/content/drive/MyDrive/res2.txt\",\"r\").read().splitlines()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1n-UaK-OUbIj"},"outputs":[],"source":["# golds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6ywnpF4vHX8"},"outputs":[],"source":["# aligns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTHfYiiQUKYs"},"outputs":[],"source":["# score_multiple([read_alignments(golds)],[read_alignments(golds)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkyUJT_q795M"},"outputs":[],"source":["\"\"\"\n","Copyright 2019 Brian Thompson\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","    https://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License.\n","\"\"\"\n","\n","import logging\n","import sys\n","from ast import literal_eval\n","from collections import OrderedDict\n","from math import ceil\n","from time import time\n","\n","import numpy as np\n","\n","import pyximport\n","pyximport.install(setup_args={'include_dirs':np.get_include()}, inplace=True, reload_support=True)\n","\n","from drive.MyDrive.dp_core import make_dense_costs, score_path, sparse_dp, make_sparse_costs, dense_dp\n","\n","logger = logging.getLogger('vecalign')  # set up in vecalign.py\n","\n","\n","def preprocess_line(line):\n","    line = line.strip()\n","    if len(line) == 0:\n","        line = 'BLANK_LINE'\n","    return line\n","\n","\n","def yield_overlaps(lines, num_overlaps):\n","    lines = [preprocess_line(line) for line in lines]\n","    for overlap in range(1, num_overlaps + 1):\n","        for out_line in layer(lines, overlap):\n","            # check must be here so all outputs are unique\n","            out_line2 = out_line[:10000]  # limit line so dont encode arbitrarily long sentences\n","            yield out_line2\n","\n","\n","def read_in_embeddings(text_file, embed_file):\n","    \"\"\"\n","    Given a text file with candidate sentences and a corresponing embedding file,\n","       make a maping from candidate sentence to embedding index, \n","       and a numpy array of the embeddings\n","    \"\"\"\n","    sent2line = dict()\n","    with open(text_file, 'rt', encoding=\"utf-8\") as fin:\n","        for ii, line in enumerate(fin):\n","            if line.strip() in sent2line:\n","                raise Exception('got multiple embeddings for the same line')\n","            sent2line[line.strip()] = ii\n","\n","    line_embeddings = np.fromfile(embed_file, dtype=np.float32, count=-1)\n","    if line_embeddings.size == 0:\n","        raise Exception('Got empty embedding file')\n","\n","    laser_embedding_size = line_embeddings.size // len(sent2line)  # currently hardcoded to 1024\n","    if laser_embedding_size != 1024:\n","        logger.warning('expected an embedding size of 1024, got %s', laser_embedding_size)\n","    logger.info('laser_embedding_size determined to be %d', laser_embedding_size)\n","    line_embeddings.resize(line_embeddings.shape[0] // laser_embedding_size, laser_embedding_size)\n","    return sent2line, line_embeddings\n","\n","\n","def make_doc_embedding(sent2line, line_embeddings, lines, num_overlaps):\n","    \"\"\"\n","    lines: sentences in input document to embed\n","    sent2line, line_embeddings: precomputed embeddings for lines (and overlaps of lines)\n","    \"\"\"\n","\n","    lines = [preprocess_line(line) for line in lines]\n","\n","    vecsize = line_embeddings.shape[1]\n","\n","    vecs0 = np.empty((num_overlaps, len(lines), vecsize), dtype=np.float32)\n","\n","    for ii, overlap in enumerate(range(1, num_overlaps + 1)):\n","        for jj, out_line in enumerate(layer(lines, overlap)):\n","            try:\n","                line_id = sent2line[out_line]\n","            except KeyError:\n","                logger.warning('Failed to find overlap=%d line \"%s\". Will use random vector.', overlap, out_line)\n","                line_id = None\n","\n","            if line_id is not None:\n","                vec = line_embeddings[line_id]\n","            else:\n","                vec = np.random.random(vecsize) - 0.5\n","                vec = vec / np.linalg.norm(vec)\n","\n","            vecs0[ii, jj, :] = vec\n","\n","    return vecs0\n","\n","\n","def make_norm1(vecs0):\n","    \"\"\"\n","    make vectors norm==1 so that cosine distance can be computed via dot product\n","    \"\"\"\n","    for ii in range(vecs0.shape[0]):\n","        for jj in range(vecs0.shape[1]):\n","            norm = np.sqrt(np.square(vecs0[ii, jj, :]).sum())\n","            vecs0[ii, jj, :] = vecs0[ii, jj, :] / (norm + 1e-5)\n","\n","\n","def layer(lines, num_overlaps, comb=' '):\n","    \"\"\"\n","    make front-padded overlapping sentences\n","    \"\"\"\n","    if num_overlaps < 1:\n","        raise Exception('num_overlaps must be >= 1')\n","    out = ['PAD', ] * min(num_overlaps - 1, len(lines))\n","    for ii in range(len(lines) - num_overlaps + 1):\n","        out.append(comb.join(lines[ii:ii + num_overlaps]))\n","    return out\n","\n","\n","def read_alignments(fin):\n","    alignments = []\n","    with open(fin, 'rt', encoding=\"utf-8\") as infile:\n","        for line in infile:\n","            fields = [x.strip() for x in line.split(':') if len(x.strip())]\n","            if len(fields) < 2:\n","                raise Exception('Got line \"%s\", which does not have at least two \":\" separated fields' % line.strip())\n","            try:\n","                src = literal_eval(fields[0])\n","                tgt = literal_eval(fields[1])\n","            except:\n","                raise Exception('Failed to parse line \"%s\"' % line.strip())\n","            alignments.append((src, tgt))\n","\n","    # I know bluealign files have a few entries entries missing,\n","    #   but I don't fix them in order to be consistent previous reported scores\n","    return alignments\n","\n","\n","def print_alignments(alignments, scores=None, file=sys.stdout):\n","    if scores is not None:\n","        for (x, y), s in zip(alignments, scores):\n","            print('%s:%s:%.6f' % (x, y, s), file=file)\n","    else:\n","        for x, y in alignments:\n","            print('%s:%s' % (x, y), file=file)\n","\n","\n","class DeletionKnob(object):\n","    \"\"\"\n","    A good deletion penalty is dependent on normalization, and probably language, domain, etc, etc\n","    I want a way to control deletion penalty that generalizes well...\n","    Sampling costs and use percentile seems to work fairly well.\n","    \"\"\"\n","    def __init__(self, samp, res_min, res_max):\n","\n","        self.res_min = res_min\n","        self.res_max = res_max\n","\n","        if self.res_min >= self.res_max:\n","            logger.warning('res_max <= res_min, increasing it')\n","            self.res_max = self.res_min + 1e-4\n","\n","        num_bins = 1000\n","        num_pts = 30\n","\n","        self.hist, self.bin_edges = np.histogram(samp, bins=num_bins,\n","                                                 range=[self.res_min, self.res_max],\n","                                                 density=True)\n","\n","        dx = self.bin_edges[1] - self.bin_edges[0]\n","        self.cdf = np.cumsum(self.hist) * dx\n","\n","        interp_points = [(0, self.res_min), ]\n","        for knob_val in np.linspace(0, 1, num_pts - 1)[1:-1]:\n","            cdf_idx = np.searchsorted(self.cdf, knob_val)\n","            cdf_val = self.res_min + cdf_idx / float(num_bins) * (self.res_max - self.res_min)\n","            interp_points.append((knob_val, cdf_val))\n","        interp_points.append((1, self.res_max))\n","        self.x, self.y = zip(*interp_points)\n","\n","    def percentile_frac_to_del_penalty(self, knob_val):\n","        del_pen = np.interp([knob_val], self.x, self.y)[0]\n","        return del_pen\n","\n","\n","def make_alignment_types(max_alignment_size):\n","    # return list of all (n,m) where n+m <= this\n","    alignment_types = []\n","    for x in range(1, max_alignment_size):\n","        for y in range(1, max_alignment_size):\n","            if x + y <= max_alignment_size:\n","                alignment_types.append((x, y))\n","    return alignment_types\n","\n","\n","def ab2xy_w_offset(aa, bb_idx, bb_offset):\n","    bb_from_side = bb_idx + bb_offset[aa]\n","    xx = aa - bb_from_side\n","    yy = bb_from_side\n","    return (xx, yy)\n","\n","\n","def xy2ab_w_offset(xx, yy, bb_offset):\n","    aa = xx + yy\n","    bb_from_side = yy\n","    bb = bb_from_side - bb_offset[aa]\n","    return aa, bb\n","\n","\n","def process_scores(scores, alignments):\n","    # floating point sometimes gives negative numbers, which is a little unnerving ...\n","    scores = np.clip(scores, a_min=0, a_max=None)\n","\n","    for ii, (x_algn, y_algn) in enumerate(alignments):\n","        # deletion penalty is pretty arbitrary, just report 0\n","        if len(x_algn) == 0 or len(y_algn) == 0:\n","            scores[ii] = 0.0\n","        # report sores un-normalized by alignment sizes\n","        #    (still normalized with random vectors, though)\n","        else:\n","            scores[ii] = scores[ii] / len(x_algn) / len(y_algn)\n","\n","    return scores\n","\n","\n","def sparse_traceback(a_b_csum, a_b_xp, a_b_yp, b_offset, xsize, ysize):\n","    alignments = []\n","    xx = xsize\n","    yy = ysize\n","\n","    cum_costs = []\n","\n","    while True:\n","        aa, bb = xy2ab_w_offset(xx, yy, b_offset)\n","\n","        cum_costs.append(a_b_csum[aa, bb])\n","\n","        xp = a_b_xp[aa, bb]\n","        yp = a_b_yp[aa, bb]\n","\n","        if xx == yy == 0:\n","            break\n","\n","        if xx < 0 or yy < 0:\n","            raise Exception('traceback bug')\n","\n","        x_side = list(range(xx - xp, xx))\n","        y_side = list(range(yy - yp, yy))\n","        alignments.append((x_side, y_side))\n","\n","        xx = xx - xp\n","        yy = yy - yp\n","\n","    alignments.reverse()\n","    cum_costs.reverse()\n","    costs = np.array(cum_costs[1:]) - np.array(cum_costs[:-1])\n","    # \"costs\" are scaled by x_alignment_size * y_alignment_size\n","    #     and the cost of a deletion is del_penalty\n","    # \"scores\": 0 for deletion/insertion, \n","    #    and cosine distance, *not* scaled \n","    #    by len(x_alignment)*len(y_alignment)\n","    scores = process_scores(scores=costs, alignments=alignments)\n","\n","    return alignments, scores\n","\n","\n","def dense_traceback(x_y_tb):\n","    xsize, ysize = x_y_tb.shape\n","\n","    xx = xsize - 1\n","    yy = ysize - 1\n","\n","    alignments = []\n","    while True:\n","        if xx == yy == 0:\n","            break\n","        bp = x_y_tb[xx, yy]\n","        if bp == 0:\n","            xp, yp = 1, 1\n","            alignments.append(([xx - 1], [yy - 1]))\n","        elif bp == 1:\n","            xp, yp = 0, 1\n","            alignments.append(([], [yy - 1]))\n","        elif bp == 2:\n","            xp, yp = 1, 0\n","            alignments.append(([xx - 1], []))\n","        else:\n","            raise Exception('got unknown value')\n","\n","        xx = xx - xp\n","        yy = yy - yp\n","\n","    alignments.reverse()\n","\n","    return alignments\n","\n","\n","def append_slant(path, xwidth, ywidth):\n","    \"\"\"\n","    Append quantized approximation to a straight line\n","       from current x,y to a point at (x+xwidth, y+ywidth)\n","    \"\"\"\n","    NN = xwidth + ywidth\n","    xstart, ystart = path[-1]\n","    for ii in range(1, NN + 1):\n","        x = xstart + round(xwidth * ii / NN)\n","        y = ystart + round(ywidth * ii / NN)\n","        # In the case of ties we want them to round differently,\n","        #   so explicitly make sure we take a step of 1, not 0 or 2\n","        lastx, lasty = path[-1]\n","        delta = x + y - lastx - lasty\n","        if delta == 1:\n","            path.append((x, y))\n","        elif delta == 2:\n","            path.append((x - 1, y))\n","        elif delta == 0:\n","            path.append((x + 1, y))\n","\n","\n","def alignment_to_search_path(algn):\n","    \"\"\"\n","    Given an alignment, make searchpath.\n","    Searchpath must step exactly one position in x XOR y at each time step.\n","    \n","    In the case of a block of deletions, the order found by DP is not meaningful.\n","    To make things consistent and to improve the probability of recovering \n","       from search errors, we search an approximately straight line\n","       through a block of deletions. We do the same through a many-many \n","       alignment, even though we currently don't refine a many-many alignment...\n","    \"\"\"\n","    path = [(0, 0), ]\n","    xdel, ydel = 0, 0\n","    ydel = 0\n","    for x, y in algn:\n","        if len(x) and len(y):\n","            append_slant(path, xdel, ydel)\n","            xdel, ydel = 0, 0\n","            append_slant(path, len(x), len(y))\n","        elif len(x):\n","            xdel += len(x)\n","        elif len(y):\n","            ydel += len(y)\n","\n","    append_slant(path, xdel, ydel)\n","\n","    return path\n","\n","\n","def extend_alignments(course_alignments, size0, size1):\n","    \"\"\"\n","    extend alignments to include new endpoints size0, size1\n","    if alignments are larger than size0/size1, raise exception\n","    \"\"\"\n","    # could be a string of deletions or insertions at end, so cannot just grab last one\n","    xmax = 0  # maximum x value in course_alignments\n","    ymax = 0  # maximum y value in course_alignments\n","    for x, y in course_alignments:\n","        for xval in x:\n","            xmax = max(xmax, xval)\n","        for yval in y:\n","            ymax = max(ymax, yval)\n","\n","    if xmax > size0 or ymax > size1:\n","        raise Exception('asked to extend alignments but already bigger than requested')\n","\n","    # do not duplicate xmax/ymax, do include size0/size1 \n","    extra_x = list(range(xmax + 1, size0 + 1))\n","    extra_y = list(range(ymax + 1, size1 + 1))\n","\n","    logger.debug('extending alignments in x by %d and y by %d', len(extra_x), len(extra_y))\n","\n","    if len(extra_x) == 0:\n","        for yval in extra_y:\n","            course_alignments.append(([], [yval]))\n","    elif len(extra_y) == 0:\n","        for xval in extra_x:\n","            course_alignments.append(([xval], []))\n","    else:\n","        course_alignments.append((extra_x, extra_y))\n","\n","\n","def upsample_alignment(algn):\n","    def upsample_one_alignment(xx):\n","        return list(range(min(xx) * 2, (max(xx) + 1) * 2))\n","\n","    new_algn = []\n","    for xx, yy in algn:\n","        if len(xx) == 0:\n","            for yyy in upsample_one_alignment(yy):\n","                new_algn.append(([], [yyy]))\n","        elif len(yy) == 0:\n","            for xxx in upsample_one_alignment(xx):\n","                new_algn.append(([xxx], []))\n","        else:\n","            new_algn.append((upsample_one_alignment(xx), upsample_one_alignment(yy)))\n","    return new_algn\n","\n","\n","def make_del_knob(e_laser,\n","                  f_laser,\n","                  e_laser_norms,\n","                  f_laser_norms,\n","                  sample_size):\n","    e_size = e_laser.shape[0]\n","    f_size = f_laser.shape[0]\n","\n","    if e_size > 0 and f_size > 0 and sample_size > 0:\n","\n","        if e_size * f_size < sample_size:\n","            # dont sample, just compute full matrix\n","            sample_size = e_size * f_size\n","            x_idxs = np.zeros(sample_size, dtype=np.int32)\n","            y_idxs = np.zeros(sample_size, dtype=np.int32)\n","            c = 0\n","            for ii in range(e_size):\n","                for jj in range(f_size):\n","                    x_idxs[c] = ii\n","                    y_idxs[c] = jj\n","                    c += 1\n","        else:\n","            # get random samples\n","            x_idxs = np.random.choice(range(e_size), size=sample_size, replace=True).astype(np.int32)\n","            y_idxs = np.random.choice(range(f_size), size=sample_size, replace=True).astype(np.int32)\n","\n","        # output\n","        random_scores = np.empty(sample_size, dtype=np.float32)\n","\n","        score_path(x_idxs, y_idxs,\n","                   e_laser_norms, f_laser_norms,\n","                   e_laser, f_laser,\n","                   random_scores, )\n","\n","        min_score = 0\n","        max_score = max(random_scores)  # could bump this up... but its probably fine\n","\n","    else:\n","        # Not much we can do here...\n","        random_scores = np.array([0.0, 0.5, 1.0])  # ???\n","        min_score = 0\n","        max_score = 1  # ????\n","\n","    del_knob = DeletionKnob(random_scores, min_score, max_score)\n","\n","    return del_knob\n","\n","\n","def compute_norms(vecs0, vecs1, num_samples, overlaps_to_use=None):\n","    # overlaps_to_use = 10  # 10 matches before\n","\n","    overlaps1, size1, dim = vecs1.shape\n","    overlaps0, size0, dim0 = vecs0.shape\n","    assert (dim == dim0)\n","\n","    if overlaps_to_use is not None:\n","        if overlaps_to_use > overlaps1:\n","            raise Exception('Cannot use more overlaps than provided. You may want to re-run make_verlaps.py with a larger -n value')\n","    else:\n","        overlaps_to_use = overlaps1\n","\n","    samps_per_overlap = ceil(num_samples / overlaps_to_use)\n","\n","    if size1 and samps_per_overlap:\n","        # sample other size (from all overlaps) to compre to this side\n","        vecs1_rand_sample = np.empty((samps_per_overlap * overlaps_to_use, dim), dtype=np.float32)\n","        for overlap_ii in range(overlaps_to_use):\n","            idxs = np.random.choice(range(size1), size=samps_per_overlap, replace=True)\n","            random_vecs = vecs1[overlap_ii, idxs, :]\n","            vecs1_rand_sample[overlap_ii * samps_per_overlap:(overlap_ii + 1) * samps_per_overlap, :] = random_vecs\n","\n","        norms0 = np.empty((overlaps0, size0), dtype=np.float32)\n","        for overlap_ii in range(overlaps0):\n","            e_laser = vecs0[overlap_ii, :, :]\n","            sim = np.matmul(e_laser, vecs1_rand_sample.T)\n","            norms0[overlap_ii, :] = 1.0 - sim.mean(axis=1)\n","\n","    else:  # no samples, no normalization\n","        norms0 = np.ones((overlaps0, size0)).astype(np.float32)\n","\n","    return norms0\n","\n","\n","def downsample_vectors(vecs1):\n","    a, b, c = vecs1.shape\n","    half = np.empty((a, b // 2, c), dtype=np.float32)\n","    for ii in range(a):\n","        # average consecutive vectors\n","        for jj in range(0, b - b % 2, 2):\n","            v1 = vecs1[ii, jj, :]\n","            v2 = vecs1[ii, jj + 1, :]\n","            half[ii, jj // 2, :] = v1 + v2\n","        # compute mean for all vectors\n","        mean = np.mean(half[ii, :, :], axis=0)\n","        for jj in range(0, b - b % 2, 2):\n","            # remove mean\n","            half[ii, jj // 2, :] = half[ii, jj // 2, :] - mean\n","    # make vectors norm==1 so dot product is cosine distance\n","    make_norm1(half)\n","    return half\n","\n","\n","def vecalign(vecs0,\n","             vecs1,\n","             final_alignment_types,\n","             del_percentile_frac,\n","             width_over2,\n","             max_size_full_dp,\n","             costs_sample_size,\n","             num_samps_for_norm,\n","             norms0=None,\n","             norms1=None):\n","    if width_over2 < 3:\n","        logger.warning('width_over2 was set to %d, which does not make sense. increasing to 3.', width_over2)\n","        width_over2 = 3\n","\n","    # make sure input embeddings are norm==1\n","    make_norm1(vecs0)\n","    make_norm1(vecs1)\n","\n","    # save off runtime stats for summary\n","    runtimes = OrderedDict()\n","\n","    # Determine stack depth\n","    s0, s1 = vecs0.shape[1], vecs1.shape[1]\n","    max_depth = 0\n","    while s0 * s1 > max_size_full_dp ** 2:\n","        max_depth += 1\n","        s0 = s0 // 2\n","        s1 = s1 // 2\n","\n","    # init recursion stack\n","    # depth is 0-based (full size is 0, 1 is half, 2 is quarter, etc)\n","    stack = {0: {'v0': vecs0, 'v1': vecs1}}\n","\n","    # downsample sentence vectors\n","    t0 = time()\n","    for depth in range(1, max_depth + 1):\n","        stack[depth] = {'v0': downsample_vectors(stack[depth - 1]['v0']),\n","                        'v1': downsample_vectors(stack[depth - 1]['v1'])}\n","    runtimes['Downsample embeddings'] = time() - t0\n","\n","    # compute norms for all depths, add sizes, add alignment types\n","    t0 = time()\n","    for depth in stack:\n","        stack[depth]['size0'] = stack[depth]['v0'].shape[1]\n","        stack[depth]['size1'] = stack[depth]['v1'].shape[1]\n","        stack[depth]['alignment_types'] = final_alignment_types if depth == 0 else [(1, 1)]\n","\n","        if depth == 0 and norms0 is not None:\n","            if norms0.shape != vecs0.shape[:2]:\n","                print('norms0.shape:', norms0.shape)\n","                print('vecs0.shape[:2]:', vecs0.shape[:2])\n","                raise Exception('norms0 wrong shape')\n","            stack[depth]['n0'] = norms0\n","        else:\n","            stack[depth]['n0'] = compute_norms(stack[depth]['v0'], stack[depth]['v1'], num_samps_for_norm)\n","\n","        if depth == 0 and norms1 is not None:\n","            if norms1.shape != vecs1.shape[:2]:\n","                print('norms1.shape:', norms1.shape)\n","                print('vecs1.shape[:2]:', vecs1.shape[:2])\n","                raise Exception('norms1 wrong shape')\n","            stack[depth]['n1'] = norms1\n","        else:\n","            stack[depth]['n1'] = compute_norms(stack[depth]['v1'], stack[depth]['v0'], num_samps_for_norm)\n","\n","    runtimes['Normalize embeddings'] = time() - t0\n","\n","    # Compute deletion penalty for all depths\n","    t0 = time()\n","    for depth in stack:\n","        stack[depth]['del_knob'] = make_del_knob(e_laser=stack[depth]['v0'][0, :, :],\n","                                                 f_laser=stack[depth]['v1'][0, :, :],\n","                                                 e_laser_norms=stack[depth]['n0'][0, :],\n","                                                 f_laser_norms=stack[depth]['n1'][0, :],\n","                                                 sample_size=costs_sample_size)\n","        stack[depth]['del_penalty'] = stack[depth]['del_knob'].percentile_frac_to_del_penalty(del_percentile_frac)\n","        logger.debug('del_penalty at depth %d: %f', depth, stack[depth]['del_penalty'])\n","    runtimes['Compute deletion penalties'] = time() - t0\n","    tt = time() - t0\n","    logger.debug('%d x %d full DP make features: %.6fs (%.3e per dot product)',\n","                 stack[max_depth]['size0'], stack[max_depth]['size1'], tt,\n","                 tt / (stack[max_depth]['size0'] + 1e-6) / (stack[max_depth]['size1'] + 1e-6))\n","    # full DP at maximum recursion depth\n","    t0 = time()\n","    stack[max_depth]['costs_1to1'] = make_dense_costs(stack[max_depth]['v0'],\n","                                                      stack[max_depth]['v1'],\n","                                                      stack[max_depth]['n0'],\n","                                                      stack[max_depth]['n1'])\n","\n","    runtimes['Full DP make features'] = time() - t0\n","    t0 = time()\n","    _, stack[max_depth]['x_y_tb'] = dense_dp(stack[max_depth]['costs_1to1'], stack[max_depth]['del_penalty'])\n","    stack[max_depth]['alignments'] = dense_traceback(stack[max_depth]['x_y_tb'])\n","    runtimes['Full DP'] = time() - t0\n","\n","    # upsample the path up to the top resolution\n","    compute_costs_times = []\n","    dp_times = []\n","    upsample_depths = [0, ] if max_depth == 0 else list(reversed(range(0, max_depth)))\n","    for depth in upsample_depths:\n","        if max_depth > 0:  # upsample previoius alignment to current resolution\n","            course_alignments = upsample_alignment(stack[depth + 1]['alignments'])\n","            # features may have been truncated when downsampleing, so alignment may need extended\n","            extend_alignments(course_alignments, stack[depth]['size0'], stack[depth]['size1'])  # in-place\n","        else:  # We did a full size 1-1 search, so search same size with more alignment types\n","            course_alignments = stack[0]['alignments']\n","\n","        # convert couse alignments to a searchpath\n","        stack[depth]['searchpath'] = alignment_to_search_path(course_alignments)\n","\n","        # compute ccosts for sparse DP\n","        t0 = time()\n","        stack[depth]['a_b_costs'], stack[depth]['b_offset'] = make_sparse_costs(stack[depth]['v0'], stack[depth]['v1'],\n","                                                                                stack[depth]['n0'], stack[depth]['n1'],\n","                                                                                stack[depth]['searchpath'],\n","                                                                                stack[depth]['alignment_types'],\n","                                                                                width_over2)\n","\n","        tt = time() - t0\n","        num_dot_products = len(stack[depth]['b_offset']) * len(stack[depth]['alignment_types']) * width_over2 * 2\n","        logger.debug('%d x %d sparse DP (%d alignment types, %d window) make features: %.6fs (%.3e per dot product)',\n","                     stack[max_depth]['size0'], stack[max_depth]['size1'],\n","                     len(stack[depth]['alignment_types']), width_over2 * 2,\n","                     tt, tt / (num_dot_products + 1e6))\n","\n","        compute_costs_times.append(time() - t0)\n","        t0 = time()\n","        # perform sparse DP\n","        stack[depth]['a_b_csum'], stack[depth]['a_b_xp'], stack[depth]['a_b_yp'], \\\n","        stack[depth]['new_b_offset'] = sparse_dp(stack[depth]['a_b_costs'], stack[depth]['b_offset'],\n","                                                 stack[depth]['alignment_types'], stack[depth]['del_penalty'],\n","                                                 stack[depth]['size0'], stack[depth]['size1'])\n","\n","        # performace traceback to get alignments and alignment scores\n","        # for debugging, avoid overwriting stack[depth]['alignments']\n","        akey = 'final_alignments' if depth == 0 else 'alignments'\n","        stack[depth][akey], stack[depth]['alignment_scores'] = sparse_traceback(stack[depth]['a_b_csum'],\n","                                                                                stack[depth]['a_b_xp'],\n","                                                                                stack[depth]['a_b_yp'],\n","                                                                                stack[depth]['new_b_offset'],\n","                                                                                stack[depth]['size0'],\n","                                                                                stack[depth]['size1'])\n","        dp_times.append(time() - t0)\n","\n","    runtimes['Upsample DP compute costs'] = sum(compute_costs_times[:-1])\n","    runtimes['Upsample DP'] = sum(dp_times[:-1])\n","\n","    runtimes['Final DP compute costs'] = compute_costs_times[-1]\n","    runtimes['Final DP'] = dp_times[-1]\n","\n","    # log time stats\n","    max_key_str_len = max([len(key) for key in runtimes])\n","    for key in runtimes:\n","        if runtimes[key] > 5e-5:\n","            logger.info(key + ' took ' + '.' * (max_key_str_len + 5 - len(key)) + ('%.4fs' % runtimes[key]).rjust(7))\n","\n","    return stack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v755onFGUR9e"},"outputs":[],"source":["# from dp_utils import yield_overlaps\n","from sentence_transformers import SentenceTransformer\n","from math import ceil\n","from random import seed as seed\n","import numpy as np\n","# from dp_utils import make_alignment_types, \\\n","    # read_in_embeddings, make_doc_embedding, vecalign\n","# from score import score_multiple\n","from ast import literal_eval\n","\n","\n","\n","model = SentenceTransformer('sentence-transformers/LaBSE')\n","#model = SentenceTransformer('/home/mh-jafari/va/LaBSE', device='cuda')\n","#model = SentenceTransformer('/home/mh-jafari/va/LaBSE')\n","#model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n","#model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n","#model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n","def read_alignments(infile):\n","    alignments = []\n","    for line in infile:\n","            fields = [x.strip() for x in line.split(':') if len(x.strip())]\n","            if len(fields) < 2:\n","                raise Exception('Got line \"%s\", which does not have at least two \":\" separated fields' % line.strip())\n","            try:\n","                src = literal_eval(fields[0])\n","                tgt = literal_eval(fields[1])\n","            except:\n","                raise Exception('Failed to parse line \"%s\"' % line.strip())\n","            alignments.append((src, tgt))\n","\n","    # I know bluealign files have a few entries entries missing,\n","    #   but I don't fix them in order to be consistent previous reported scores\n","    return alignments\n","\n","def print_alignments(alignments, scores=None):\n","    res = []\n","    if scores is not None:\n","        for (x, y), s in zip(alignments, scores):\n","            res.append('%s:%s:%.6f' % (x, y, s))\n","    else:\n","        for x, y in alignments:\n","            res.append('%s:%s' % (x, y))\n","    return res\n","\n","def overlaps(lines, num_overlaps=50):\n","    output = set()\n","    for out_line in yield_overlaps(lines, num_overlaps):\n","        output.add(out_line)\n","\n","    # for reproducibility\n","    output = list(output)\n","    output.sort()\n","\n","    return list(output)\n","\n","def embed_sentences(sentences):\n","    embeddings = model.encode(sentences)\n","    return embeddings\n","\n","def read_in_embeddings(sentences):\n","    sent2line = dict()\n","    ol = overlaps(sentences)\n","    for ii, line in enumerate(ol):\n","        if line.strip() in sent2line:\n","            raise Exception('got multiple embeddings for the same line')\n","        sent2line[line.strip()] = ii\n","\n","    vecs = embed_sentences(ol)\n","    return sent2line, vecs\n","\n","def align(\n","    source_text,\n","    target_text,\n","    gold_alignment=None,\n","    alignment_max_size=50,\n","    del_percentile_frac=0.1,\n","    max_size_full_dp=300,\n","    costs_sample_size=100000,\n","    num_samps_for_norm=1000,\n","    search_buffer_size=5,\n","):\n","\n","    src_sent2line, src_line_embeddings = read_in_embeddings(source_text)\n","    tgt_sent2line, tgt_line_embeddings = read_in_embeddings(target_text)\n","\n","    width_over2 = ceil(alignment_max_size / 2.0) + search_buffer_size\n","\n","    test_alignments = []\n","    stack_list = []\n","\n","    src_lines = source_text\n","    vecs0 = make_doc_embedding(src_sent2line, src_line_embeddings,\n","                                src_lines, alignment_max_size)\n","\n","    tgt_lines = target_text\n","    vecs1 = make_doc_embedding(tgt_sent2line, tgt_line_embeddings,\n","                                tgt_lines, alignment_max_size)\n","\n","    final_alignment_types = make_alignment_types(alignment_max_size)\n","\n","    stack = vecalign(vecs0=vecs0,\n","                        vecs1=vecs1,\n","                        final_alignment_types=final_alignment_types,\n","                        del_percentile_frac=del_percentile_frac,\n","                        width_over2=width_over2,\n","                        max_size_full_dp=max_size_full_dp,\n","                        costs_sample_size=costs_sample_size,\n","                        num_samps_for_norm=num_samps_for_norm)\n","\n","    # write final alignments to stdout\n","    out = {}\n","    out['alignment'] = print_alignments(stack[0]['final_alignments'],\n","                        stack[0]['alignment_scores'])\n","\n","\n","    if gold_alignment is not None:\n","        test_alignments=stack[0]['final_alignments']\n","        gold_list = read_alignments(gold_alignment)\n","        res = score_multiple(gold_list=[gold_list], test_list=[test_alignments])\n","        out['score'] = res\n","        # log_final_scores(res)\n","\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMkRI3TQ9e6X"},"outputs":[],"source":["t = align(sentences1,sentences2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6HWmhAymIu5_"},"outputs":[],"source":["res = t['alignment']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-NwgV0akWxH"},"outputs":[],"source":["golds = open(\"/content/drive/MyDrive/gold.txt\",\"r\").read().splitlines()\n","\n","aligns = open(\"/content/drive/MyDrive/res5.txt\",\"r\").read().splitlines()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1680538414609,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":-210},"id":"5Od3j_9QI9nW","outputId":"3c95c91b-3bed-42b5-c010-25d074f3c8c9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[0]:[4]', '[1]:[5,6]', '[2]:[7,8]', '[3]:[9]', '[5,6]:[10]', '[7]:[11]']"]},"metadata":{},"execution_count":27}],"source":["golds"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1680538414610,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":-210},"id":"2UWeOWnTWAKE","outputId":"ceac5238-9044-42fc-8e63-412c36d37e29"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[0]:[0]', '[1]:[1]', '[2]:[2]', '[3]:[5]']"]},"metadata":{},"execution_count":28}],"source":["aligns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1680538414611,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":-210},"id":"J8LIF7MRenN0","outputId":"5f9e50d4-4768-416c-b657-002eac545b97"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[0]:[0]:0.056577',\n"," '[1]:[1]:0.072640',\n"," '[2]:[2]:0.106081',\n"," '[]:[3]:0.000000',\n"," '[]:[4]:0.000000',\n"," '[3]:[5]:0.000026']"]},"metadata":{},"execution_count":29}],"source":["res"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1680538414611,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":-210},"id":"3f38u7B7FRjZ","outputId":"39a333fa-7b4e-4a47-c3fa-4ba302bd9df2"},"outputs":[{"output_type":"stream","name":"stdout","text":["[([0], [0]), ([1], [1]), ([2], [2]), ([3], [5])]\n","[([0], [0]), ([1], [1]), ([2], [2]), ([3], [5])]\n"]},{"output_type":"execute_result","data":{"text/plain":["{'recall_strict': 1.0,\n"," 'recall_lax': 1.0,\n"," 'precision_strict': 1.0,\n"," 'precision_lax': 1.0,\n"," 'f1_strict': 1.0,\n"," 'f1_lax': 1.0}"]},"metadata":{},"execution_count":30}],"source":["score_multiple([read_alignments(aligns)],[read_alignments(aligns)])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1680538414613,"user":{"displayName":"Nikki M","userId":"12345895666301116120"},"user_tz":-210},"id":"9rs-kFTHLx9v","outputId":"97c1a0f0-e8a4-4685-987d-0dab83023ba1"},"outputs":[{"output_type":"stream","name":"stdout","text":["[([0], [0]), ([1], [1]), ([2], [2]), ([3], [5])]\n","[([0], [0]), ([1], [1]), ([2], [2]), ([3], [5])]\n"]},{"output_type":"execute_result","data":{"text/plain":["{'recall_strict': 1.0,\n"," 'recall_lax': 1.0,\n"," 'precision_strict': 1.0,\n"," 'precision_lax': 1.0,\n"," 'f1_strict': 1.0,\n"," 'f1_lax': 1.0}"]},"metadata":{},"execution_count":31}],"source":["score_multiple([read_alignments(res)],[read_alignments(aligns)])"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyMZ7w0GGzyhdEvdyF5X7pek"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}